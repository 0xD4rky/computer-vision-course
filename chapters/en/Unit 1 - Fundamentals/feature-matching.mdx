# Feature Matching

How can we match detected features from image-to-image? Feature matching involves comparing key features or characteristics in different
images to find similarities. For example, you might want to match features between two images to identify objects or recognize patterns.

## Brute-Force Search

Imagine you have a giant box of puzzle pieces, and you're trying to find a specific piece that fits
into your puzzle. This is similar to searching for matching features in images. Instead of having any special strategy,
you decide to check every single piece, one by one, until you find the right one. This straightforward method is like a brute force search.
The advantage of brute force is its simplicity. You don't need any special tricks â€“ just patience. However, it can be time-consuming,
especially if there are a lot of pieces to check. In the context of feature matching, this brute force approach is akin to comparing every pixel in one image to every pixel
in another to see if they match. It's exhaustive and it might take a lot of time, especially for large images.

OK, now that we have an intuitive idea of how brute force matches are found, let's dive into the algorithms.

_Brute Force with ORB descriptors_

1.  Load images

```python
    img1 = cv.imread('image1.png',cv.IMREAD_GRAYSCALE)
    img2 = cv.imread('image2.png',cv.IMREAD_GRAYSCALE)
```

2.  Intialize the ORB descriptor:

```python
orb = cv.ORB_create()
```

4. Find key points and descriptors:

```python
kp1, des1 = orb.detectAndCompute(img1,None)
kp2, des2 = orb.detectAndCompute(img2,None)
```

3.  Because ORB is a binary descriptor, we ind matches using Hamming Distance, which is a measure of the difference between
    two strings of equal length

```python
bf = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)
```

4.  Find the matches

```python
matches = bf.match(des1,des2)
```

5. Sort them in the order of their distance.

````python
matches = sorted(matches, key = lambda x:x.distance)
```

6. Draw first n matches.
```python
img3 = cv.drawMatches(img1,kp1,img2,kp2,matches[:n],None,flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
```

_Brute Force with SIFT_

1.  Load Images
```python
img1 = cv.imread('image1.png',cv.IMREAD_GRAYSCALE)
img2 = cv.imread('image2.png',cv.IMREAD_GRAYSCALE)
```

2. Initiate SIFT detector
```python
sift = cv.SIFT_create()
```

3. Find the keypoints and descriptors with SIFT
```python
kp1, des1 = sift.detectAndCompute(img1,None)
kp2, des2 = sift.detectAndCompute(img2,None)
```

4. Find matches using k nearest neighbors
```python
bf = cv.BFMatcher()
matches = bf.knnMatch(des1,des2,k=2)
```

5. Apply ratio test to threshold the best matches
```python
good = []
for m,n in matches:
    if m.distance < 0.75*n.distance:
        good.append([m])
```

5. Draw the matches
```python
img3 = cv.drawMatchesKnn(img1,kp1,img2,kp2,good,None,flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
```

![](URL)

## Fast Library for Approximate Nearest Neighbors (FLANN)

FLANN was proposed in [Fast Approximate Nearest Neighbors With Automatic Algorithm Configuration](https://www.cs.ubc.ca/research/flann/uploads/FLANN/flann_visapp09.pdf) by Muja and Lowe.
To explain FLANN, we will continue with our puzzle solving example. Visualize a giant puzzle with hundreds of pieces scattered around.
Your goal is to organize these pieces based on how well they fit together. Instead of randomly trying to match pieces,
FLANN uses some clever tricks to quickly figure out which pieces are most likely to go together.
Instead of trying every piece against every other piece, FLANN streamlines the process by finding pieces that are approximately similar.
This means it can make educated guesses about which pieces might fit well together, even if they're not an exact match. Under the hood, FLANN is uses something called k-D trees.
Think of it as organizing the puzzle pieces in a special way. Instead of checking every piece against every other piece, FLANN arranges them in a tree-like structure that makes finding matches faster.
In each node of the k-D tree, FLANN puts pieces with similar features together. It's like sorting puzzle pieces with similar shapes or colors into piles.
This way, when you're looking for a match, you can quickly check the pile that's most likely to have similar pieces. Let's say you're looking for a "sky" piece.
Instead of searching through all the pieces, FLANN guides you to the right spot in the k-D tree where the sky-colored pieces are sotred. FLANN also adjusts its strategy based on the features of the puzzle pieces.
If you have a puzzle with lots of colors, it will focus on color features. Alternately, if it's a puzzle with intricate shapes, it pays attention to those shapes.
By balancing speed and accuracy when finding matching features, FLANN substantially improves query time.

_First we create a dictionary to specify the algorithm we will use:_

- For SIFT or SURF
```python
FLANN_INDEX_KDTREE = 1
index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
```

- For ORB
```python
FLANN_INDEX_LSH = 6
index_params= dict(algorithm = FLANN_INDEX_LSH,
                   table_number = 12,
                   key_size = 20,
                   multi_probe_level = 2)
```

_We also create a dictionary to specify the maximum leafs to visit:_

```python
search_params = dict(checks=50)
```

1.  Load images
```python
img1 = cv.imread('image1.png',cv.IMREAD_GRAYSCALE)
img2 = cv.imread('image2.png',cv.IMREAD_GRAYSCALE)

2. Initiate SIFT detector
```python
sift = cv.SIFT_create()
```

3. Find the keypoints and descriptors with SIFT
```python
kp1, des1 = sift.detectAndCompute(img1,None)
kp2, des2 = sift.detectAndCompute(img2,None)
```

3. Define the FLANN parameters.  Trees is the numbers of bins you want.

```python
FLANN_INDEX_KDTREE = 1
index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
search_params = dict(checks=50)
flann = cv.FlannBasedMatcher(index_params,search_params)

matches = flann.knnMatch(des1,des2,k=2)
```

4. We will only draw only good matches, so create a mask

```python
matchesMask = [[0,0] for i in range(len(matches))]
```

5. We perform a ratio test to determine good matches
```python
for i,(m,n) in enumerate(matches):
    if m.distance < 0.7*n.distance:
        matchesMask[i]=[1,0]
```

6. Now we visualize the matches
```python
draw_params = dict(matchColor = (0,255,0),
                   singlePointColor = (255,0,0),
                   matchesMask = matchesMask,
                   flags = cv.DrawMatchesFlags_DEFAULT)

img3 = cv.drawMatchesKnn(img1,kp1,img2,kp2,matches,None,**draw_params)
```

![](URL)

## Local Feature Matching with Transformers (LoFTR)

LoFTR was proposed in [LoFTR: Detector-Free Local Feature Matching with Transformers](https://arxiv.org/pdf/2104.00680.pdf) by Sun, et. al.
Instead of using feature detectors, LoFTR uses learning-based approach to feature matching.

Let's keep it simple and use our puzzle example once again. As before, each piece is analogous to unique features in the images-such as corners, edges, or distinct patterns.
Instead of simply comparing images pixel by pixel, LoFTR looks for specific key points, or features, in each image. It's like identifying the corners and edges of each puzzle piece.
Just as someone really good a putting together a puzzle might focus on distinctive marks, LoFTR identifies these unique points in one image. These could be key landmarks or structures that stand out.
What makes LoFTR advanced is its ability to handle challenges like rotation and scaling. If a puzzle piece is turned or resized, LoFTR can still recognize it. It's like solving puzzles where pieces may be flipped or adjusted.
As LoFTR matches features, it assigns a similarity score to indicate how well the features align. Higher scores mean better matches. It's like giving a grade to how well one puzzle piece fits with another.
LoFTR is also invariant to certain transformations, meaning it can handle variations in lighting, angle, or perspective. This is crucial when dealing with images that might be photographed under different conditions.
LoFTR's ability to robustly match features makes it valuable for tasks like image stitching, where you combine multiple images seamlessly by identifying and connecting common features.

We can use [Kornia](https://github.com/kornia/kornia) to find matching features in two images using LoFTR.

![](URL)

## Resources and Further Reading

[FLANN Github](https://github.com/flann-lib/flann)
[FLANN - Fast Library for Approximate Nearest Neighbors - User Manual](https://www.cs.ubc.ca/research/flann/uploads/FLANN/flann_manual-1.8.4.pdf)
[Guide Local Feature Matching by Overlap Estimation]
[Image Matching Using SIFT, SURF, BRIEF and
ORB: Performance Comparison for Distorted Images](https://arxiv.org/pdf/1710.02726.pdf)
[Kornia tutorial on Image Matching](https://kornia.github.io/tutorials/nbs/image_matching.html)
[LoFTR Github](https://github.com/zju3dv/LoFTR)
[OpenCV Github](https://github.com/opencv/opencv-python)
[OpenGlue: Open Source Graph Neural Net Based Pipeline for Image Matching](https://arxiv.org/abs/2204.08870)
````
