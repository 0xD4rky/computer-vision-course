# Feature Matching

How can we match detected features from image-to-image? Feature matching involves comparing key features or characteristics in different
images to find similarities. For example, you might want to match features between two images to identify objects or recognize patterns.

## Brute-Force Search

Imagine you have a giant box of puzzle pieces, and you're trying to find a specific piece that fits
into your puzzle. This is similar to searching for matching features in images. Instead of having any special strategy,
you decide to check every single piece, one by one, until you find the right one. This straightforward method is like a brute force search.
The advantage of brute force is its simplicity. You don't need any special tricks â€“ just patience. However, it can be time-consuming,
especially if there are a lot of pieces to check. In the context of feature matching, this brute force approach is akin to comparing every pixel in one image to every pixel
in another to see if they match. It's exhaustive and it might take a lot of time, especially for large images.

## Fast Library for Approximate Nearest Neighbors (FLANN)

FLANN was proposed in [Fast Approximate Nearest Neighbors With Automatic Algorithm Configuration](https://www.cs.ubc.ca/research/flann/uploads/FLANN/flann_visapp09.pdf) by Muja and Lowe.
To explain FLANN, we will continue with our puzzle solving example. Visualize a giant puzzle with hundreds of pieces scattered around.
Your goal is to organize these pieces based on how well they fit together. Instead of randomly trying to match pieces,
FLANN uses some clever tricks to quickly figure out which pieces are most likely to go together.
Instead of trying every piece against every other piece, FLANN streamlines the process by finding pieces that are approximately similar.
This means it can make educated guesses about which pieces might fit well together, even if they're not an exact match. Under the hood, FLANN is uses something called k-D trees.
Think of it as organizing the puzzle pieces in a special way. Instead of checking every piece against every other piece, FLANN arranges them in a tree-like structure that makes finding matches faster.
In each node of the k-D tree, FLANN puts pieces with similar features together. It's like putting pieces with similar shapes or colors into buckets.
This way, when you're looking for a match, you can quickly check the bucket that's most likely to have similar pieces. Let's say you're looking for a "sky" piece. You won't have to search through all the puzzple pieces.
Instead, FLANN guides you to the right bucket in the k-D tree where the sky-colored pieces are. FLANN is also adjusts its strategy based on the features of the puzzle pieces.
If you have a puzzle with lots of colors, it will focus on color features. Alternately, if it's a puzzle with intricate shapes, it pays attention to those shapes.
In a nut-shell, FLANN tries to balance spped and accuracy when finding matching features.

## Local Feature Matching with Transformers (LoFTR)

LoFTR was proposed in [LoFTR: Detector-Free Local Feature Matching with Transformers](https://arxiv.org/pdf/2104.00680.pdf) by Sun, et. al.
Instead of using feature detectors, LoFTR uses learning-based approach to feature matching.

Let's keep it simple and use our puzzle example once again. As before, each piece is analogous to unique features in the images-such as corners, edges, or distinct patterns.
Instead of simply comparing images pixel by pixel, LoFTR looks for specific key points, or features, in each image. It's like identifying the corners and edges of each puzzle piece.
Just as someone really good a putting together a puzzle might focus on distinctive marks, LoFTR identifies these unique points in one image. These could be key landmarks or structures that stand out.
What makes LoFTR advanced is its ability to handle challenges like rotation and scaling. If a puzzle piece is turned or resized, LoFTR can still recognize it. It's like solving puzzles where pieces may be flipped or adjusted.
As LoFTR matches features, it assigns a similarity score to indicate how well the features align. Higher scores mean better matches. It's like giving a grade to how well one puzzle piece fits with another.
LoFTR is also invariant to certain transformations, meaning it can handle variations in lighting, angle, or perspective. This is crucial when dealing with images that might be photographed under different conditions.
LoFTR's ability to robustly match features makes it valuable for tasks like image stitching, where you combine multiple images seamlessly by identifying and connecting common features.

We can use [Kornia](https://github.com/kornia/kornia) to find matching features in two images using LoFTR.

![](URL)

## Resources:

[FLANN Github](https://github.com/flann-lib/flann)
[Guide Local Feature Matching by Overlap Estimation]
[Kornia tutorial on Image Matching](https://kornia.github.io/tutorials/nbs/image_matching.html)
[LoFTR Github](https://github.com/zju3dv/LoFTR)
[OpenGlue: Open Source Graph Neural Net Based Pipeline for Image Matching](https://arxiv.org/abs/2204.08870)
