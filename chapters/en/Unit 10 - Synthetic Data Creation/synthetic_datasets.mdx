

# **MAJOR SYNTHETIC DATASETS**

In this section, we will cover the most popular synthetic datasets in computer vision, categorized into specific use cases and providing real-case usage examples.

As the field moved from classical unsupervised approaches to deep learning, state of the art models began to require large datasets that could not be pro- duced in real life, and after the transition to deep learning synthetic datasets started to dominate. 

[TOC]



## Low Level Computer Vision Problems

### **Optical Flow and Motion Analysis**

Estimating the distribution of apparent velocities of movement along the image.

 Optical flow is important for motion estimation and video compression.

| Dataset Name | Year                  | Description                                                  | Paper                                                        | Code | Additional Links                                             |
| ------------ | --------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---- | ------------------------------------------------------------ |
| Middlebury   | 2021 (latest release) | The **Middlebury** Stereo dataset consists of high-resolution stereo sequences with complex geometry and pixel-accurate ground-truth disparity data. The ground-truth disparities are acquired using a technique that employs structured lighting and does not require the calibration of the light projectors. | [A database and  evaluation method for Optical Flow](https://vision.middlebury.edu/flow/floweval-ijcv2011.pdf) (Cited by 3192 at the time of writing) |      | [Papers with Code](https://paperswithcode.com/dataset/middlebury) - [Project](https://vision.middlebury.edu/stereo/data/) |
| MPI-Sintel   | 2012                  | A synthetic dataset for optical flow. The main characteristic feature of MPI-Sintel is that it contains the same scenes with different render settings, varying quality and complexity; this approach can provide a deeper understanding of where different optical flow algorithms break down. (paper quote) |                                                              |      | [Paper](https://link.springer.com/chapter/10.1007/978-3-642-33783-3_44), [Project](http://sintel.is.tue.mpg.de/) |
| SceneFlow    | N/A                   | N/A                                                          |                                                              |      | [Project (Inactive)](https://www.ri.cmu.edu/project/scene-flow/) |

---



### Stereo Image Matching

Finding the correspondence between the points of two images of the same scene from different viewpoints

| Dataset Name     | Year | Description                                                  | Paper                                                        | Code | Additional Links                                       |
| ---------------- | ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---- | ------------------------------------------------------ |
| Tsukuba Stereo   | 2012 | This dataset includes 1800 stereo pairs accompanied by ground truth disparity maps, occlusion maps, and discontinuity maps. |                                                              |      | [Project](https://home.cvlab.cs.tsukuba.ac.jp/dataset) |
| Middlebury 2014  | 2014 | 33 high-res stereo datasets                                  | [High-resolution stereo datasets with subpixel-accurate ground truth](https://link.springer.com/chapter/10.1007/978-3-319-11752-2_3) |      |                                                        |
| Flying Chairs    | 2015 | 22k frame pairs with ground truth flow                       |                                                              |      |                                                        |
| Flying Chairs 3D | 2015 | 22k stereo frames                                            |                                                              |      |                                                        |
| Monkaa           | 2015 | 8591 stereo frames                                           |                                                              |      |                                                        |
| Driving          | 2015 | 4392 stereo frames.                                          |                                                              |      |                                                        |
|                  |      |                                                              |                                                              |      |                                                        |





## 3D Objects datasets

Basic high-level computer vision problems, such as object detection or segmen- tation, fully enjoy the benefits of perfect labeling provided by synthetic data, and there is plenty of effort devoted to making synthetic data work for these problems. Since making synthetic data requires the development of 3D mod- els, datasets usually also feature 3D-related labeling such as the depth map, labeled 3D parts of a shape, volumetric 3D data, and so on.

| Dataset        | Year | Description                                         | Paper                                                        | Additional Links                |
| -------------- | ---- | --------------------------------------------------- | ------------------------------------------------------------ | ------------------------------- |
| YCB            | 2015 | 77 objects in 5 categories                          | Unrealcv: Virtual worlds for com- puter vision.              |                                 |
| ShapeNet       | 2015 | >3M models, 3135 categories, rich annotations       | Shapenet: An information-rich 3d model repository.           |                                 |
| ShapeNetCore   | 2017 | 51K manually verified models from 55 categories     | Large-scale 3d shape reconstruction and segmentation from shapenet core55. |                                 |
| UnrealCV       | 2017 | Plugin for UE4 to generate synthetic data           | Unrealcv: Virtual worlds for com- puter vision.              |                                 |
| VANDAL         | 2017 | 4.1M depth images, >9K objects in 319 categories    | [63†87]                                                      |                                 |
| SceneNet       | 2015 | Automated indoor synthetic data generator           | [64†232]                                                     |                                 |
| SceneNet RGB-D | 2017 | 5M RGB-D images from 16K 3D trajectories            | [65†401]                                                     |                                 |
| DepthSynth     | 2017 | Framework for realistic simulation of depth sensors | [66†455]                                                     |                                 |
| PartNet        | 2018 | 26671 models, 573535 annotated part instances       | [67†415]                                                     |                                 |
| Falling Things | 2018 | 61.5K images of YCB objects in virtual envs         | [68†594]                                                     |                                 |
| ADORESet       | 2019 | Hybrid dataset for object recognition testing       | [69†42]                                                      |                                 |
| 3DScan         | 2016 | a large dataset of object scans                     | https://arxiv.org/abs/1602.02481                             | http://redwood-data.org/3dscan/ |
| seeing3DChairs |      |                                                     |                                                              |                                 |



## High Level Computer Vision Problems



### **Autonomous Driving**

| Name        | Year | Description | Paper |  | Code | Additional Links |
|---------------------|--------------|-------------|----------------|---------------------|---------------------|---------------------|
| Virtual KITTI 2      | 2020          | Virtual Worlds as Proxy for Multi-Object Tracking Analysis |  [Project](https://europe.naverlabs.com/Research/Computer-Vision/Proxy-Virtual-Worlds/) |  |  | [Papers With Code](https://paperswithcode.com/dataset/virtual-kitti-2) |
| Driving in the Matrix | 2017       | The core idea behind "Driving in the Matrix" is to use photo-realistic computer-generated images from a simulation engine to quickly produce annotated data.  | [Paper](https://arxiv.org/pdf/1610.01983.pdf) |  | [GitHub](https://github.com/umautobots/driving-in-the-matrix) (104 stars at the time of writing) | [Project](https://fcav.engin.umich.edu/projects/driving-in-the-matrix), [Papers With Code](https://paperswithcode.com/paper/driving-in-the-matrix-can-virtual-worlds) |
| Synthia             | 2016         | A large collection of synthetic images for semantic segmentation of urban scenes. SYNTHIA consists of a collection of photo-realistic frames rendered from a virtual city and comes with precise pixel-level semantic annotations for 13 classes: misc, sky, building, road, sidewalk, fence, vegetation, pole, car, sign, pedestrian, cyclist, lane-marking. | [Paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Ros_The_SYNTHIA_Dataset_CVPR_2016_paper.html) |  |  | [Project](https://synthia-dataset.net/) |
| GTA5 | 2016 | The **GTA5** dataset contains 24966 synthetic images with pixel level semantic annotation. The images have been rendered using the open-world video game **Grand Theft Auto 5** and are all from the car perspective in the streets of American-style virtual cities. There are 19 semantic classes which are compatible with the ones of Cityscapes dataset. | [Playing for Data: Ground Truth from Computer Games](https://arxiv.org/abs/1608.02192v1) |  | [Code](https://bitbucket.org/visinf/projects-2016-playing-for-data/src/master/) | [Project](https://download.visinf.tu-darmstadt.de/data/from_games/index.html) - [Papers with code](https://paperswithcode.com/dataset/gta5) |

---

### **Indoor Simulation and Navigation**

| Dataset Name | Release Year | Description | External Links |
|--------------|--------------|-------------|----------------|
| Minos        | 2017          | Multimodal Indoor Simulator | [GitHub](https://github.com/minosworld/minos) |
| House3D      | Archived 2021-08-28 | A Rich and Realistic 3D Environment | [GitHub](https://github.com/facebookresearch/House3D) |

---

### **Human Action Recognition and Simulation**

| Dataset Name | Release Year | Description                                                  | External Links                                               |
| ------------ | ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| PHAV         | N/A          | Synthetic dataset of procedurally generated human action recognition videos | [Project](http://adas.cvc.uab.es/phav/), [Paper](https://openaccess.thecvf.com/content_cvpr_2017/papers/de_Souza_Procedural_Generation_of_CVPR_2017_paper.pdf) |
| Surreal      | 2017         | (change description - this is for human depth estimation and human part segmentation) Large-scale dataset with synthetically-generated but realistic images of people  rendered from 3D sequences of human motion capture data. We generate more than 6 million frames together with ground truth pose, depth maps, and segmentation masks. We show that CNNs trained on our synthetic dataset allow for accurate human depth estimation and human part segmentation in real RGB images. | [Code](https://github.com/gulvarol/surreal), [Paper](https://arxiv.org/abs/1701.01370), [Project](https://www.di.ens.fr/willow/research/surreal/) |

---



---

### **Scene Understanding and Semantic Segmentation**

| Dataset Name | Release Year | Description | External Links |
|--------------|--------------|-------------|----------------|
| SunCG (Princeton) | 2017      | Semantic Scene | [Paper](https://sscnet.cs.princeton.edu/), [Papers With Code](https://paperswithcode.com/dataset/suncg) |

---

### **Diverse Applications**

| Dataset Name          | Release Year | Description | External Links |
|-----------------------|--------------|-------------|----------------|
| 4D Light Field Dataset| N/A          | N/A | [GitHub](https://github.com/lightfield-analysis), [Paper](https://lightfield-analysis.net/benchmark/paper/lightfield_benchmark_accv_2016.pdf), [Project](https://lightfield-analysis.uni-konstanz.de/) |
| ICL-NUIM Dataset      | N/A          | The













-----

## Other synthetic datasets to categorise 

| Dataset Name             | Application | External Links |
|--------------------------|--------------|----------------|
| ProcSy                 |              |                                                              |
| Playing for benchmarks (2017) |          | [Project](https://playing-for-benchmarks.org/overview/), [Paper](https://vladlen.info/papers/playing-for-benchmarks.pdf) |
| Pix3D |  |  |
| FaceSynthetics | Face Recognition |  |
| FFHQ | Face Recognition |  |
| Carionfacial Datasets | Face Recognition |  |
| ABO |  |  |
| CIFAKE |  |  |
| YCB-Video | Object detection and recognition |  |
| ApolloScape | Semantic Segmentation |  |
| CARLA | Autonomous Driving |  |
| NTIRE | Image Super resolution |  |
|  |  |  |





### Additional information on already listed datasets (some of them have already been integrated in the tables above)

This section contains some additional information that might be interesting to report (example: github stars and paper citations)

- SunCG (Princeton)
    - Semantic Scene
    - [currently not available](https://paperswithcode.com/dataset/suncg)
    - [Paper: Semantic Scene Completion from a Single Depth Image](https://sscnet.cs.princeton.edu/)
    
- 

- House3D
    - Archived on August 28, 2021
    - A Rich and R3ealistic 3D Environment
    - [GitHub Repository](https://github.com/facebookresearch/House3D)
    - House3D is a virtual 3D environment which consists of thousands of indoor scenes equipped with a diverse set of scene types, layouts and objects sourced from the SUNCG dataset. It consists of over 45k indoor 3D scenes, ranging from studios to two-storied houses with swimming pools and fitness rooms. All 3D objects are fully annotated with category labels. Agents in the environment have access to observations of multiple modalities, including RGB images, depth, segmentation masks and top-down 2D map views. The renderer runs at thousands frames per second, making it suitable for large-scale RL training.
    
- PHAV (Procedural Human Action Videos)
    - Synthetic dataset of procedurally generated human action recognition videos
    - [Project](http://adas.cvc.uab.es/phav/)
    - [Paper](https://openaccess.thecvf.com/content_cvpr_2017/papers/de_Souza_Procedural_Generation_of_CVPR_2017_paper.pdf)
    
    
    
- Sintel
    - A synthetic dataset for optical flow
    - [Paper: A naturalistic Open Source Movie for Optical Flow Evaluation](https://link.springer.com/chapter/10.1007/978-3-642-33783-3_44)
    - 551 citations
    - 2012
    - [Project](http://sintel.is.tue.mpg.de/)
    
- SceneFlow
    - [This project is no longer active](https://www.ri.cmu.edu/project/scene-flow/)
    
- 4D Light Field Dataset
    - [GitHub Project](https://github.com/lightfield-analysis)
    - [Paper: A Dataset and Evaluation Methodology for Depth Estimation on 4D Light Fields](https://lightfield-analysis.net/benchmark/paper/lightfield_benchmark_accv_2016.pdf)
    - [Project](https://lightfield-analysis.uni-konstanz.de/)
    - Per scene, we provide:
        - 9x9x512x512x3 light fields as individual PNGs
        - Config files with camera settings and disparity ranges
        - Per center view (except for the 4 test scenes):
            - 512x512 and 5120x5120 depth and disparity maps as PFMs
            - 512x512 and 5120x5120 evaluation masks as PNGs
    - We further provide depth and disparity maps for all 81 views of the additional scenes.
    - For file format descriptions and read/write utilities, see our Matlab and Python scripts.
    
- ICL-NUIM Dataset
    - The ICL-NUIM dataset aims at benchmarking RGB-D, Visual Odometry and SLAM algorithms. Two different scenes (the living room and the office room scene) are provided with ground truth. Living room has 3D surface ground truth together with the depth-maps as well as camera poses and as a result pefectly suits not just for bechmarking camera trajectory but also reconstruction. Office room scene comes with only trajectory data and does not have any explicit 3D model with it. Below we provide data for four different handheld trajectories that are obtained by running Kintinuous on real image data, and finally used in synthetic framework for obtaining ground truth.
    - [Project](https://www.doc.ic.ac.uk/~ahanda/VaFRIC/iclnuim.html)
    
- Playing for benchmarks (this one is for both high level and low level tasks.)
    - We present a benchmark suite for visual perception. The benchmark is based on more than 250K high-resolution video frames, all annotated with ground-truth data for both low-level and high-level vision tasks, including optical flow, semantic instance segmentation, object detection and tracking, object-level 3D scene layout, and visual odometry. Ground-truth data for all tasks is available for every frame. The data was collected while driving, riding, and walking a total of 184 kilometers in diverse ambient conditions in a realistic virtual world. To create the benchmark, we have developed a new approach to collecting ground-truth data from simulated worlds without access to their source code or content. We conduct statistical analyses that show that the composition of the scenes in the benchmark closely matches the composition of corresponding physical environments. The realism of the collected data is further validated via perceptual experiments. We analyze the performance of state-of-the-art methods for multiple tasks, providing reference baselines and highlighting challenges for future research.
    - [Project](https://playing-for-benchmarks.org/overview/)
    - [Paper: Playing for benchmark](https://vladlen.info/papers/playing-for-benchmarks.pdf)
    - 2017
    
    



We could add a subsection on synthetic data generators. 




# References 

[Repository "Synthetic Data for Computer Vision"](https://github.com/unrealcv/synthetic-computer-vision)

https://arxiv.org/pdf/1909.11512.pdf - overview synthetic data generation
