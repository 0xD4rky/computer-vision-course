# Introduction

Besides fundamentals, this unit assumes familiarity with concepts in transfer and multimodal learning. If you are not viewing the course sequentially in the provided order, we suggest at least reading the transfer learning section in unit 2, and the whole of unit 4.

## On Generalization

We have now trained our model like a student cramming all epochs for a test. Now, the real test begins! We hope this knowledge translates beyond the specific pictures it learned from, allowing it to recognize unseen cats like Alice's and Ted’s furry friends. Think of it as the model learning the essence of catness, not just those specific furry faces it saw during training. This ability to apply knowledge to new situations is called generalization, and it's what separates a good model from a mere cat picture memorizer. Can you imagine an alternate universe without generalization? Yes, it’s pretty simple actually, you’ll only have to train your model on ALL the images of cats in the world (assuming they only exist on earth), including Alice’s and Ted’s.

Actually, I wasn’t totally accurate when I said that the model is expected to generalize to all cat pictures that it has not seen. It is expected to generalize to all cat pictures that come from the same distribution as the image data it was trained on. Simply, if you trained your model on cat selfies and then presented it with a cartoon picture of a cat, it probably won’t be able to recognize it. These two pictures come from totally different distributions or domains. Making your cat selfies model able to recognize cartoon cats is referred to as domain adaptation (we’ll briefly talk about it later). It's like taking all the knowledge your model learned about real cats and teaching it to recognize their animated cousins.

So, we’ve gone from generalization, recognizing the unseen Alice’s and Ted’s cat pictures, to domain adaptation, recognizing animated cat pictures. But we’re much Greedier than that. You don’t want your model to be able to only recognize your cat pictures, or Alice’s and Ted’s, or not even cartoon cat! Having a model trained on cat pictures, you also want it to recognize pictures of llamas and falcons.

Well, now we’re in the turf of Zero-shot Learning. Also known as, ZSL.

[comment]: # (TODO: Illustration showing the difference between generalization, domain adaptation, and ZSL)

## What is Zero-shot Learning?

Let’s warm up with a definition

Zero-shot learning is a setup in which the model is presented with images belonging to **only** classes that it was not exposed to during training, at test time. In other words, the training and testing sets are **disjoint**.

Just a heads-up: in the classic ZSL setup, the test set only has pictures of classes the model hasn't seen before, not a single one from its training days. This may seem a little bit unrealistic, it's  like asking a student to ace an exam on only materials they've never studied. 

Luckily, there's a more pragmatic version of ZSL that doesn't have this strict rule and is called generalized zero-shot learning, or GZSL. This more flexible approach allows the test set to include both seen and unseen classes. It's a more realistic scenario, reflecting how things work in the real world.

[comment]: # (TODO: Illustration showing the difference between ZSL and GZSL)

## History of Zero-shot Learning 

The question of whether we can make models that perform well on tasks which they were not explicitly trained on, came soon after deep learning started to seem feasible. In 2008, the seeds of ZSL were sowed with two independent papers presented at the Association for the Advancement of Artificial Intelligence (AAAI) conference. The first paper, titled **Dataless Classification**, explored the concept of zero-shot learning in the context of natural language processing (NLP). The second one, titled **Zero-Data Learning**, focused on applying ZSL to computer vision tasks. The term zero-shot learning itself first appeared in 2009 at NeurIPS in a paper co-authored by, wait for it….**Geoff Hinton**.

Let’s have a nice outline for the most important moments in ZSL below

| 2008 | The first sparks of (the question of) zero-shot learning           |
| 2009 | The term zero-shot learning was coined                             |
| 2013 | The concept of Generalized ZSL was introduced                      |
| 2017 | The first application of the encoder-decoder paradigm to ZSL       |
| 2020 | OpenAI’s GPT3 achieves an impressive performance on zero-shot NLP  |
| 2021 | OpenAI’s CLIP takes zero-shot computer vision to a whole new level |

The impact of CLIP has been profound, ushering in a new era of zero-shot computer vision research. Its versatility and performance have opened up exciting new possibilities. It could be said that the history of zero-shot computer vision can be divided into two eras: The **pre-CLIP** and **post-CLIP** eras.

## How Does Zero-shot Learning Work In Computer Vision?

Now that we know what zero-shot learning is, it would be nice to know how it is applied to computer vision, right? This part will be addressed in more detail in the coming chapters, but let’s paint a broad picture here to break the ice.

In NLP, zero-shot learning is (although it wasn’t always the case) pretty straight-forward. Many language models are trained on massive datasets of text, learning to predict the next word in a given sequence. This ability to capture the underlying patterns and semantic relationships within language allows these models to perform surprisingly well on tasks they weren’t explicitly trained on. A really nice example is GPT2 being able to summarize a passage when TL;DR is appended to the prompt. Zero-shot computer vision is another story.

Let’s begin by answering a simple question, *how can we humans do it? How can we recognize objects that we have not seen before?*

Yes, you’re totally right! We need some **other information** about that object. We can easily identify a tiger, even if we haven’t seen one before, given that we have a knowledge of what a lion looks like. A tiger is a lion with stripes and minus the mane. A zebra is a black and white striped version of a horse. Deadpool is a red-black version of Spiderman.

As we can see, zero-shot computer vision is essentially multi-modal. If generalization is like learning a language by reading books and talking to people. Zero-shot computer vision is like learning a language by reading a dictionary and listening to someone describe what it sounds like.

### What Other Information?

It is called **Semantic or Auxiliary Information**. Semantic information embeds both seen and unseen classes in high dimensional vectors, and it comes in many different forms:

1. **Attribute vectors**: Thinks of attributes as a tabular representation of the different features for the object. 
2. **Textual Descriptions**: A text describing the object contained in the image similar to image captions.
3. **Class label vectors**: Simply using the embeddings of class labels.

[comment]: # (TODO: Illustration showing the different forms of semantic information)

And using this semantic information, you train a model to learn a mapping function between the image features and the semantic features. At inference time, the model predicts the class label by looking for the closest label in the semantic space by using, for example, k-nearest neighbor. We can say that we are transferring knowledge obtained from seen classes with the help of semantic information.

Different zero-shot computer vision methods differ in what **semantic information** they use and which **embedding space** they utilize at inference.

[comment]: # (TODO: Illustration showing the pipeline of zero-shot learning)

### How Is This Different from Transfer Learning?

Good question! Zero-shot learning (ZSL) falls within the broader category of transfer learning, specifically under the umbrella of **heterogeneous transfer learning**. This means that ZSL relies on transferring knowledge gained from a **source domain** (seen classes) to a *distinct* **target domain** (unseen classes). 

Due to the disparity between the source and target domains, ZSL faces specific challenges in transfer learning. These include overcoming domain shift, where the data distributions differ significantly, and handling the lack of labeled data in the target domain. But we’re going to talk about these challenges in detail later, so don’t worry about it now. For now, let’s talk a little about the different methods of zero-shot computer vision.
