# Zero-shot Learning


## 1. ZSL vs. Generalized ZSL

Zero-shot learning (ZSL) and generalized zero-shot learning (GZSL) both belong to a type of machine learning algorithm, where the image classification model needs to classify labels not included in training. ZSL and GZSL are very similar, and the main difference is how the model is evlauated.

For ZSL, the model is purely evaluated for its capability to classify images of unseen classes - only observations of unseen classes are included in ZSL testing dataset. As for GZSL, the model is evaluated on both seen classes and unseen classes - which is considered more practical and closer to real-world usecases. Overall, GZSL is more challenging, because the model needs to determine if an observation belongs to a novel class or a known class. 

As descrived in previous section, to develop a successful ZSL model requires more than just images and class labels. It is near impossible to classify unseen classes based on images alone. ZSL utilizes semantic embeddings to help classify images from unseen classes. 

### Semantic Embeddings 

Semantic embeddings are vector representations of semantic information - the meaning and interpretation of textual data. For example, the information transferred with spoken text is a type of semantic information. Semantic information does not include only the direct meanings of words or sentences, but also the contextual and cultural implications.

Embeddings refers to the process to map semantic information into vectors of real numbers. Semantic embeddings are often learned with unsupervised machine learning models, such as Word2Vec [1] or GloVe [2]. All types of textual information such as words, phrases, or sentences can be transformed into numerical vectors based on set procedures. Semantic embeddings describe words in a high-dimensional space where the distance and direction between words reflect their semantic relationships. This enables machines to understand the usage, synonyms, and context of each word by mathematical operations on the word embeddings. 


## 2. Types of ZSL Based on Training Data

Based on the types of training data, there are two kinds of zero-shot learning:

### Inductive zero shot

Training data includes labelled images of seen classes, and semantic descriptions/attributes of both seen and unseen classes. The main idea is to establish projections from observaed image space to semantic latent space, and the model can identify unseen classes at inference time.

### Transductive zero shot

Training data includes labelled images of seen classes and unlabelled images of unseen classes, and semantic descriptions or attributes of both seen and unseen classes. The usecase for transductive learning is when we do not have enough labeling resource to label all images.


## 3. Zero Shot Learning Example 

DeViSE - A Deep Visual-Semantic Embedding Model [1]

Developed in 2013 by Frome et al., the authors from Google presented a method to leverage Word2Vec semantic information to train an image classifiction model on 1000-class ImageNet data. The model has 3 phases:

- 1) Language Model Pretraining 

A skip-gram word embedding model of 500-1000 dimensions were trained with Wikipedia articles, similar to processes described in Word2Vec development [2].  

- 2) Visual Model Pretraining 

AlexNet was applied as starting model architecture for training an image classification model with ILSVRC data [3].  

- 3) Deep Visual-Semantic Embedding Model 

An image classification model - the core visual model - is trained with labels (Fig 1). Next, a transformation layer after the output of core visual model is trained along with the embedding vectors of image labels to incraese the dot product similarity. At inference time, when a new image arrives, the output of core visual model and the transformation layer is first computed, and the nearest label in semantic embedding space will be the predicted label for the image.  

- Results

DeViSE can successfully predict a wide range of labels outside of training data. The absolute accuracy is not high, and is only 9% under strict conditions. However, it should nonetheless be considered good since the number of test image labels is ~1000.  

- Example Code 

```

```


## 4. Benchmarks and Evaluation

The benchmarks and standard datasets are:

1）Animal with Attributes（AwA）
2）Caltech-UCSD-Birds-200-2011（CUB）
3）Sun database（SUN）
4）Attribute Pascal and Yahoo dataset（aPY）
5）ILSVRC2012/ILSVRC2010（ImNet-2）


## Reference

- [1] Mikilov et al., Efficient estimation of word-representations in vector space, ICLR (2013). 
- [2] Pennington et al., Glove: Global vectors for word representation, EMNLP (2014).
- [3] Frome et al., DeViSE: A deep visualsemantic embedding model, NIPS, (2013)
- [4] Deng et al., Imagenet: A large-scale hierarchical image datbse, CVPR (2012). 
- [5] Pourpanah et al., A Review of Generalized Zero-Shot Learning Methods (2022). 
