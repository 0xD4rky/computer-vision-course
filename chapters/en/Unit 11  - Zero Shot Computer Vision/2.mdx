# Zero-shot Learning

## Zero-shot Learning vs. Generalized Zero-shot Learning

Zero-shot learning (ZSL) and generalized zero-shot learning (GZSL) belong to a machine learning algorithm type, where the image classification model needs to classify labels not included in training. ZSL and GZSL are very similar, and the main difference is how the model is evaluated.

For ZSL, the model is purely evaluated for its capability to classify images of unseen classes - only observations of unseen classes are included in ZSL testing dataset. As for GZSL, the model is evaluated on both seen classes and unseen classes - which is considered more practical and closer to real-world use cases. Overall, GZSL is more challenging because the model needs to determine if an observation belongs to a novel class or a known class. 

As described in the previous section, developing a successful ZSL model requires more than just images and class labels. It is nearly impossible to classify unseen classes based on images alone. ZSL utilizes semantic embeddings to help classify images from unseen classes. 

### Semantic Embeddings 

Semantic embeddings are vector representations of semantic information - the meaning and interpretation of textual data. For example, the information transferred with spoken text is a type of semantic information. Semantic information does not include only the direct meanings of words or sentences but also the contextual and cultural implications.

Embeddings refer to the process of mapping semantic information into vectors of real numbers. Semantic embeddings are often learned with unsupervised machine learning models, such as Word2Vec [1] or GloVe [2]. All types of textual information, such as words, phrases, or sentences can be transformed into numerical vectors based on set procedures. Semantic embeddings describe words in a high-dimensional space where the distance and direction between words reflect their semantic relationships. This enables machines to understand the usage, synonyms, and context of each word by mathematical operations on the word embeddings. 


## Types of ZSL Based on Training Data

Based on the types of training data, there are two kinds of zero-shot learning.

### Inductive  Zero-shot Learning

Training data includes labeled images of seen classes and semantic descriptions/attributes of both seen and unseen classes. The main idea is to establish projections from observed image space to semantic latent space, and the model can identify unseen classes at inference time.

### Transductive Zero-shot Learning

Training data includes labeled images of seen classes, and unlabeled images of unseen classes, and semantic descriptions or attributes of both seen and unseen classes. The use case for transductive learning is when we do not have enough labeling resources to label all images.


## Benchmarks and Evaluation

The benchmarks and standard datasets are:

- **Animal with Attributes (AwA)**

Dataset to benchmark transfer-learning algorithms, in particular attribute based classification [6]. It consists of 30475 images of 50 animal classes with six feature representations for each image.  

- **Caltech-UCSD-Birds-200-2011（CUB）**

Dataset for fine-grained visual categorization task. It contains 11788 images of 200 subcategories of birds. Each image has 1 subcategory label, 15 part locations, 312 binary attributes and 1 bounding box. Also, ten-sentence descriptions for each image. were collected through Mechanical Turk by Amazon, and the descriptions are carefully constructed to not contain any subcategory information.  

- **Sun database（SUN）**

Scene categorization benchmarks. It consists of 130519 images of 899 categories. 

- **Attribute Pascal and Yahoo dataset（aPY）**

A coarse-grained dataset composed of 15339 images from 3 broad categories (animals, objects and vehicles), further divided into a total of 32 subcategories. 

- **ILSVRC2012/ILSVRC2010（ImNet-2）**

The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) evaluates algorithms for object detection and image classification at large scale. 



## Reference

- [1] Mikilov et al., Efficient Estimation of Word-Representations in Vector Space, ICLR (2013). 
- [2] Pennington et al., Glove: Global Vectors for Word Representation, EMNLP (2014).
- [3] Frome et al., DeViSE: A Deep Visual Semantic Embedding Model, NIPS, (2013)
- [4] Deng et al., Imagenet: A Large-Scale Hierarchical Image Datbse, CVPR (2012). 
- [5] Pourpanah et al., A Review of Generalized Zero-Shot Learning Methods (2022).
- [6] Lampert et al., Learning to Detect Unseen Object Classes by Between-Class Attribute Transfer, CVPR (2009).
