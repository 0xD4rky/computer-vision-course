## Overview

### What is Retention Networks
RetNet, or Retentive Network, is a foundational architecture proposed for large language models in the paper [Retentive Network: A Successor to Transformer for Large Language Models](https://arxiv.org/abs/2307.08621). This architecture is designed to address key challenges in the realm of large-scale language modeling: Training Parallelism, Low-Cost Inference, and Good performance.

![LLM Challenges.png](https://huggingface.co/datasets/hf-vision/course-assets/blob/main/LLM%20Challenges.png)
RetNet is able to tackle these challenges by introducing the Multi-Scale Retention (MSR) mechanism, which is an alternative to the multi-head attention mechanism commonly used in Transformer models. 
MSR has a dual form of recurrence and parallelism, so it is possible to train the models in a parallel way while recurrently conducting inference.

The Multi-Scale Retention mechanism operates under three computation paradigms:
1) **Parallel Representation**: This aspect of RetNet is designed to fully utilize GPU resources, enabling efficient parallel training of the model.
![Parallel Representation.png](https://huggingface.co/datasets/hf-vision/course-assets/blob/main/Parallel%20Representation.png)

2) **Recurrent Representation**: This representation facilitates efficient inference with O(1) complexity in terms of memory and computational requirements. It significantly reduces deployment costs and latency, and simplifies implementation by eliminating the need for key-value cache strategies often used in traditional models.
![Recurrent Representation.png](https://huggingface.co/datasets/hf-vision/course-assets/blob/main/Recurrent%20Representation.png)

3) **Chunkwise Recurrent Representation**: This third paradigm addresses the challenge of long-sequence modeling. It achieves this by encoding each local block in parallel for computational speed while simultaneously encoding global blocks in a recurrent manner to optimize GPU memory usage.

During the training phase, the approach incorporates both parallel and chunkwise recurrent representations, optimizing GPU usage for fast computation and being particularly effective for long sequences in terms of computational efficiency and memory use. 
For the inference phase, the recurrent representation is used, favoring autoregressive decoding. This method efficiently reduces memory usage and latency, maintaining equivalent performance outcomes.

### From Language to Image
Paper [RMT: Retentive Networks Meet Vision Transformers](https://arxiv.org/abs/2309.11523) proposes a new vision backbone inspired by the RetNet architecture. The authors propose RMT to enhance the Vision Transformer (ViT) by introducing explicit spatial priors and reducing computational complexity, drawing inspiration from the RetNet's parallel representation. 
This includes adapting the RetNetâ€™s temporal decay to spatial domains and using a [Manhattan distance-based](https://en.wikipedia.org/wiki/Taxicab_geometry) spatial decay matrix, along with an attention decomposition form, to improve efficiency and scalability in vision tasks.

* Manhattan Self-Attention (MaSA)
![Attention Comparison.png](https://huggingface.co/datasets/hf-vision/course-assets/blob/main/Attention%20Comparison.png)
MaSA incorporates Self-Attention mechanism with a two-dimensional bidirectional spatial decay matrix based on the Manhattan distance among the tokens. This matrix decreases attention scores for tokens further away from a target token, allowing it to perceive global information while varying attention based on distance.

* Decomposed Manhattan Self-Attention (MaSAD)
![MaSAD.png](https://huggingface.co/datasets/hf-vision/course-assets/blob/main/MaSAD.png)
This mechanism decomposes Self-Attention in images along horizontal and vertical axes of the image, maintaining the spatial decay matrix without losing prior information. This decomposition allows the Manhattan Self-Attention (MaSA) to model global information efficiently with linear complexity, while preserving the original MaSA's receptive field shape.

However, unlike the original RetNet that performs training with parallel representation and inference with recurrent representation, RMT does both processes with MaSA mechanism. The authors have done comparisons between MaSA and other RetNet's representations, and they show that MaSA has the best throughput with the highest accuracy.
![MaSA vs Retention.png](https://huggingface.co/datasets/hf-vision/course-assets/blob/main/MaSA%20vs%20Retention.png)

## Further Reading

- [Retentive Networks (RetNet) Explained: The much-awaited Transformers-killer is here](https://medium.com/ai-fusion-labs/retentive-networks-retnet-explained-the-much-awaited-transformers-killer-is-here-6c17e3e8add8)
- [Retentive Network: A Successor to Transformer for Large Language Models (Paper Explained)](https://www.youtube.com/watch?v=ec56a8wmfRk)
