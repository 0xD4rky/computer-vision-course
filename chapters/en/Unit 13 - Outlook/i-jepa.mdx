# Image-based Joint-Embedding Predictive Architecture (I-JEPA)

## Overview

The Image-based Joint-Embedding Predictive Architecture (I-JEPA) is a groundbreaking self-supervised learning model [introduced by Meta AI in 2023](https://ai.meta.com/blog/yann-lecun-ai-model-i-jepa/). It tackles the challenge of understanding images without relying on traditional labels or hand-crafted data augmentations.
To get to know I-JEPA better, let’s first discuss some few concepts.

### Invariance-based vs. Generative pretraining methods

We can say that there are broadly two main approaches for self-supervised learning from images, Invariance-based methods and generative methods. Both approaches have their strengths and weaknesses.

- **Invariance-based methods**: In these methods the model tries to reproduce similar embeddings for different views of the same image. And of course these different view are hand-crafted, the image augmentations that we’re all familiar with. For example: rotating, scaling, and cropping. These methods are good at producing representations at high semantic levels, but the problem is that they introduce strong biases which may be detrimental for certain downstream tasks. For example, image classification and instance segmentation do not require the data augmentations.

- **Generative methods**: In these methods the model tries to reconstruct the input image. That’s why these methods are sometimes called reconstruction-based self-supervised learning. Masks are used to hide patches of the input image, and the model tries to reconstruct these corrupted patches either at the pixel or token level (let’s keep this point in mind). This masked approach can easily generalize beyond image modality, but dosent’t produce representations at the quality level of invariance-based methods. Also, these methods are computationally expensive and require large datasets for robust training.

Now let’s talk about Joint-Embedding Architectures.

### Joint-Embedding Architectures

This is a recent and popular approach for self-supervised learning from images in which two networks are trained to produce similar embeddings for different views of the same image. Basically, they train two networks to "speak the same language" about different views of the same picture. A common choice is the Siamese network architecture where the two networks share the same weights. But like everything else, it has its own problems:

- **Representation collapse**: A case in which the model produces the same representation regardless of the input. Or in tech-speak, the energy landscape is flat.

- **Inputs compatibility criteria**: Finding good and appropriate compatibility measures can be challenging sometimes.

An example of a Joint-Embedding Architecture is VICReg

<Tip>
Different training methods can be employed to train Joint-Embedding Architectures, for example:

- Contrastive methods
- Non-Contrastive methods
- Clustering methods
</Tip>


