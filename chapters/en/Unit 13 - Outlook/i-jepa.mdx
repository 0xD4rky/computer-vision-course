# Image-based Joint-Embedding Predictive Architecture (I-JEPA)

## Overview

The Image-based Joint-Embedding Predictive Architecture (I-JEPA) is a groundbreaking self-supervised learning model [introduced by Meta AI in 2023](https://ai.meta.com/blog/yann-lecun-ai-model-i-jepa/). It tackles the challenge of understanding images without relying on traditional labels or hand-crafted data augmentations.
To get to know I-JEPA better, let’s first discuss some few concepts.

### Invariance-based vs. Generative pretraining methods

We can say that there are broadly two main approaches for self-supervised learning from images, Invariance-based methods and generative methods. Both approaches have their strengths and weaknesses.

- **Invariance-based methods**: In these methods the model tries to reproduce similar embeddings for different views of the same image. And of course these different view are hand-crafted, the image augmentations that we’re all familiar with. For example: rotating, scaling, and cropping. These methods are good at producing representations at high semantic levels, but the problem is that they introduce strong biases which may be detrimental for certain downstream tasks. For example, image classification and instance segmentation do not require the data augmentations.

- **Generative methods**: In these methods the model tries to reconstruct the input image. That’s why these methods are sometimes called reconstruction-based self-supervised learning. Masks are used to hide patches of the input image, and the model tries to reconstruct these corrupted patches either at the pixel or token level (let’s keep this point in mind). This masked approach can easily generalize beyond image modality, but dosent’t produce representations at the quality level of invariance-based methods. Also, these methods are computationally expensive and require large datasets for robust training.

Now let’s talk about Joint-Embedding Architectures.



