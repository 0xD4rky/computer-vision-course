
# ConvNext - A ConvNet for the 2020s (2022)

## Introduction
Recently, the breakthrough of Vision Transformers (ViTs) quickly superseded pure CMM models as the new state-of-the-art for image recognition.
Intriguingly, research has found that a significant portion of the design choices in Vision Transformers can be mapped to CNNs.
ConvNext represents a significant improvement to pure convolution models by incorporating techniques inspired by ViTs and achieving results comparable to ViTs in terms of accuracy and scalability.


## Key Improvements
ConvNext starts with a regular ResNet (ResNet-50) and gradually modernizes and modifies the architecture to closely imitate the hierarchical structure of Vision Transformers.
The key improvements are:
- Training Techniques
- Macro Design
- ResNeXt-ify
- Inverted Bottleneck
- Large Kernel Sizes
- Micro Design
We will go through each of the key improvements briefly.
These designs are not novel in itself, however you can learn how researchers adapt and modify designs systematically to improve existing models.
[Block Comparison](./images/convnext/block_comparison.png)

## Training Techniques
Before changing the design of the architecture, having a good training procedure will significantly affect the performance.
ConvNext applies training techniques close to DeiT and Swin Transformers which are:
- Extended epochs from the original 90 epochs to 300 epochs
- AdamW optimizer
- Data augmentation techniques such as Mixup, Cutmix, RandAugment, Random Erasing
- Regularization schemes including Stochastic Depth and Label Smoothing
This improves ResNet-50's accuracy from 76.1% to 78.8%.


## Macro Design
Analyzing Swin Transformers' macro network, there are two interesting design considerations that is used in ConvNext.

### The stage compute ratio
The stage compute ratio refers to how the computational load is distributed among the stages of a neural network model.
Following Swin Transformer's compute ratio of 1:1:3:1, the researchers adjusted the number of blocks on each stage of the ResNet from the original (3, 4, 6, 3) to (3, 3, 9, 3). 
This improves the model accuracy from 78.8% to 79.4%.

### Changing stem to Patchify
Typically, on the start of ResNet's architecture, the input is fed to a stem a 7×7 convolution layer with stride 2, followed by a max pool, which is used to downsample the image by a factor of 4.
However, it turns out to be substitutable to a simpler but more effective 4×4 non-overlapping convolutional layer with stride 4.
This improves the model accuracy from 79.4% to 79.5%.


## ResNeXt-ify
ConvNext adopts the idea of ResNeXt, which has been explored in the previous page. 
In short, ResNeXt has a better FLOPs (number of floating point operations) over accuracy trade-off than a vanilla ResNet by having grouped convolution for the 3×3 conv layer in the bottleneck block.
Specifically, ConvNext increases the network width to 96 channels.
This improves the model accuracy from 79.5% to 80.5%.


## Inverted Bottleneck
An important idea in every Transformer block is usage of inverted bottleneck, where the hidden layers are bigger (by a ratio of 4 for ConvNext) than the input dimension. 
This idea has also been used and popularized in Computer Vision by MobileNetV2.
By using this technique, it improves the model accuracy from 80.5% to 80.6%.
[Inverted Bottleneck Comparison](./images/convnext/inverted_bottleneck.png)


## Large Kernel Sizes
One of the reasons Vision Transformer does so well is its non-local self-attention which enables a more global receptive field of image features.
Swin Transformers' attention block window size is at least 7×7, bigger than ResNext's 3x3 kernel size. ConvNext changes ResNext's original kernel size as follows.
- Moving up the position of the depthwise convolution layer is a prerequisite to increasing the kernel size. By doing this, we can let the 1x1 layers do the efficient heavy lifting while the depthwise convolution layer focus as a more non-local receptor.
- With the prerequisite set up, we can get the benefit of adopting larger kernel-sized convolutions. 
By using kernel size 7x7, the network retains the accuracy while improving the model's overall FLOPs.
[Moving up the Depth Conv Layer](./images/convnext/depthwise_moveup.png)

## Micro Design
In addition to the modifications stated, ConvNext also considers micro design, focusing on layer level details.
The changes are:
- Replacing ReLU activation with GELU (Gaussian Error Linear Unit) (optional, accuracy stays unchanged) 
- Fewer activation functions (eliminate all GELU layers from the residual block except for one between two 1×1 layers, replicating the style of a Transformer block)
- Fewer normalization layers (remove two BatchNorm layers, leaving only one BN layer before the conv 1 × 1 layers)
- Substituting Batch Normalization with Layer Normalization (LN improves the model after doing the previous modifications)
- Separate downsampling layers (add a separate downsample layer in between ResNet stages, adding LN layers before each downsampling layer)
With these final modifications, this improves the final model accuracy from 80.6% to 82.0%, exceeding Swin Transformer's accuracy (81.3%).


## Model Code
You can go to [this HuggingFace documentation](https://huggingface.co/docs/transformers/model_doc/convnext) to learn more about how to integrate the ConvNext pipeline into your code.

## References
The paper "A ConvNet for the 2020s" proposed the ConvNext architecture in 2022 by a team of researchers from Facebook AI Research, including Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.
The paper can be found [here](https://arxiv.org/abs/2201.03545) and the GitHub repo can be found [here](https://github.com/facebookresearch/ConvNeXt).