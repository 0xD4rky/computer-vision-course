# Transformer-based image segmentation

In this section, we'll explore how Vision Transformers (ViTs) compare to Convolutional Neural Networks (CNNs) in image segmentation, and detail the architecture of a ViT-based segmentation model as an example.

<Tip warning={true}>
  This section assumes familiarity with image segmentation, Convolutional Neural
  Networks (CNNs), and the basics of Vision Transformers (ViTs). If you're new
  to these concepts, we recommend exploring related materials in the course
  before proceeding.
</Tip>

## CNNs vs ViTs for Segmentation

Prior to the emergence of ViTs (Vision Transformers), CNNs (Convolutional Neural Networks) have been the go-to choice for image segmentation. Models like [U-Net](https://arxiv.org/abs/1505.04597) and [Mask R-CNN](https://arxiv.org/abs/1703.06870) capture the details that is needed to distinguish different objects in an image, making them state-of-the-art for segmentation tasks.

Despite their excellent results over the past decade, CNN-based models have some limitations, which Transformers aim to solve:

- **Spatial limitations** : CNNs learn local patterns through small receptive fields. This local focus makes it hard to "link" features that are far apart but related within the image, affecting their ability to segment complex scenes/objects accurately. Unlike CNNs, ViTs are designed to capture global dependencies within an image leveraging the attention mechanism. This means ViT-based models consider the entire image at once, allowing them to understand complex relationships between distant parts of an image. For segmentation, this global perspective can lead to more accurate delineation of objects.
- **Task-Specific Components**: Methods like Mask R-CNN incorporate hand-designed components (e.g., non-maximum suppression, spatial anchors) to encode prior knowledge about segmentation tasks. These components add complexity and require manual tuning. In contrast, ViT-based segmentation methods simplify the segmentation process by eliminating the need for hand-designed components, making them more straightforward to optimize.
- **Segmentation Task Specialization**: CNN-based segmentation models approaches semantic, instance and panoptic segmentation tasks individually, leading to specialized architectures for each task and separate research efforts into each. Recent ViT-based models like [MaskFormer](https://arxiv.org/abs/2107.06278), [SegFormer](https://arxiv.org/abs/2105.15203) or [SAM](https://arxiv.org/abs/2304.02643) provide a unified approach to tackling semantic, instance, and panoptic segmentation tasks within a single framework.

## Spotlight on MaskFormer: Illustrating ViT for Image Segmentation

MaskFormer, introduced in the paper "MaskFormer: Per-Pixel Classification is Not All You Need for Semantic Segmentation" is a model that predicts segmentation masks for each class present in an image, unifying semantic and instance segmentation in one architecture.

### Architecture overview

The figure below shows the architecture diagram taken from the paper.

<img
  width="600"
  src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/maskformer_architecture.png"
/>

The architecture is composed of three components:

**Pixel-level Module**: Uses a backbone to extract image features and a pixel decoder to generate per-pixel embeddings.

**Transformer Module**: Employs a standard Transformer decoder to compute per-segment embeddings from image features and learnable positional embeddings (queries), encoding global information about each segment.

**Segmentation Module**: Generates class probability predictions and mask embeddings for each segment using a linear classifier and a Multi-Layer Perceptron (MLP), respectively. The mask embeddings are used in combination with per-pixel embeddings to predict binary masks for each segment.

The model is trained with a binary mask loss (same one as [DETR](https://arxiv.org/abs/2005.12872)) and a cross entropy classification loss per predicted segment.

### Panoptic segmentation inference example with `transformers`ðŸ¤—

```python
from transformers import AutoImageProcessor, MaskFormerForInstanceSegmentation
from PIL import Image
import requests

image_processor = AutoImageProcessor.from_pretrained("facebook/maskformer-swin-base-coco")
model = MaskFormerForInstanceSegmentation.from_pretrained("facebook/maskformer-swin-base-coco")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = image_processor(images=image, return_tensors="pt")

outputs = model(**inputs)

class_queries_logits = outputs.class_queries_logits
masks_queries_logits = outputs.masks_queries_logits

result = image_processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]
predicted_panoptic_map = result["segmentation"]
```

## Applying Transfer learning/Finetuning to ViT-based segmentation models

With many pretrained segmentation models available, transfer learning or finetuning are used to adapt these models to specific use cases, especially since ViT-based segmentation models, like MaskFormer, are data-hungry and challenging to train from scratch.
these techniques leverages pre-trained representations to adapt these models efficiently to new tasks. Typically for MaskFormer, the backbone, the pixel decoder and the transformer decoder are kept frozen to leverage their learned general features, while the transformer module is finetuned to adapt its class prediction and mask generation capabilities to new segmentation tasks.

## References

1. [MaskFormer Hugging Face documentation](https://huggingface.co/docs/transformers/en/model_doc/maskformer)
2. [Image segmentation Transfer learning tutorial](https://colab.research.google.com/drive/1Fn3apbFXBPn5lFccQCMLmiWyQ7ZqZYBk?usp=sharing)
