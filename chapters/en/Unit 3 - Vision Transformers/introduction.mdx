# Introduction to Vision Transformers

In the recent unit we learned about Convolutional Neural Networks (CNNs) and some of their use cases. 
We saw how some of the most prominent CNNs can be implemented and fine-tuned on custom data.

CNNs have been the go-to choice for Computer Vision practicioners for many tasks ever since the advent of the AlexNet architecture. 
However, the wheel of time keeps turning and new architectures are proposed on a regular basis, that challenge the supremacy of CNNs.
While many of them fade away or are only used in niche domains, the recent years had emerge one competitor who is on the way to take 
the crown as the favourite choice for Computer Vision practicioners: Vision Transformers (ViTs).

## Transformers
The Transfomer architecture was originally proposed in the 2017 paper []"Attention is All You Need"](https://arxiv.org/abs/1706.03762) by Vaswani et al.
The novel architecture quickly caught interest in the Natural Language Processing community, which before has mainly been building its 
applications on Recurrent Neural Network (RNN) architectures like Long Short-Term Memory(LSTM).

Transformers promised a wide array of new application possibilities, as they surpassed RNN architectures in many terms, e.g. Parallelization, 
Scalability and Flexibility.

## Self-Attention
While the new architecture has many mechanisms that play together nicely, the most important of them might be Self-Attention, 
a new way of taking into account the dependency between parts of the input.

.....


### CNN vs Vision Transformers: Inductive Bias

Inductive bias is a term used in machine learning to describe the set of assumptions that a learning algorithm uses to make predictions. In simpler terms, inductive bias is like a shortcut that helps a machine learning model make educated guesses based on the information it has seen so far.

Here's a couple of inductive biases we observe in CNNs:

- Translational Equivariance: an object can appear anywhere in the image, and CNNs can detect its features.
- Locality: pixels in an image interact mainly with its surrounding pixels to form features.

These are lacking in Vision Transformers. Then how do they perform so well? It's because they're highly scalable and they're trained on massive amounts of images. Hence, they overcome the need for these inductive biases.


