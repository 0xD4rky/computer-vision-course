# Introduction

Welcome to the chapter on Fusion of Text and Vision. This chapter builds the foundation
for the later sections of the unit. We will explore:
- The notion of multimodality, and different sensory inputs humans use for efficient decision making. 
- Why is it important for making innovative applications and services through which we can interact and make lives easier.
- Multimodality in context to Deep Learning, data, tasks, and models.
- Some applications of multimodality, like multimodal emotion recognition and multimodal search.

So let's begin ü§ó

## What is multimodality? üì∏üìùüéµ

As the word suggests, modality means a medium or way in which something exists or is done. 
In our daily life, we come across many scenarios where we have to make decisions and perform tasks.
For this we use our 5 sense organs (eyes to see, ears to hear, nose to smell, tongue to taste and skin to touch).
Based on the information from all organs, we assess our environment, perform tasks and take decisions for our survival. Each of these 5 sense organs
are 5 different modalities through which information comes to us and thus the word multimodality or multimodal. 


Think about this for a moment, you receive a text on your favourite social media platform which is funny, you react using emojis ü•±üòÖü§£ forward the joke 
to your friends but not everyone understands the joke or finds it funny. You are unaware whether the joke was actually funny
and you wanted some kind of feedback (obviously some reacted but others didnot).
Then you go on to call your friend and tell this joke to the person over a phone call, and you take a good laugh on it. Now you are much
more aware of your friend's reaction, you can make sense of minute details like their voice, tone, pitch, pauses in between clearly giving you a better feedback than before. 
Finally, you go to a social gathering and tell the same joke to your friend face-to-face. Now you are completely sure about whether the joke was found to 
be funny by your friend, details like nodding or scratching their heads, feeling distracted are now completely visible. The feedback stronger than before. So what just happened?

So what just happened? In the first scenario, you had limited understanding of the situation, in the second scenario your understanding was better, but finally you were able to make 
more sense of whether the other person actually found the joke funny or not. As we kept on adding modalities our understanding of the situation became better and clearer than before,
for the same joke (you can avoid laughing now because it's still text from the course contributors ü§îüòî)

Even while taking this course and moving ahead, would you not like cool infographics, accompanied by video content explaining minute concepts instead of just plain textual content üòâ
Here you go:

![Multimodality Notion](https://cdn-lfs.huggingface.co/repos/bf/2a/bf2a50c0acddc20a4963bc4c9bfd57f4a0a887faf272014a120b3c17331f0aa6/d541378a986bf634792e98a8314518d5074bc068ce09442ba7a412e03e3c0328?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27multimodal_elephant.png%3B+filename%3D%22multimodal_elephant.png%22%3B&response-content-type=image%2Fpng&Expires=1702912379&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMjkxMjM3OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iZi8yYS9iZjJhNTBjMGFjZGRjMjBhNDk2M2JjNGM5YmZkNTdmNGEwYTg4N2ZhZjI3MjAxNGExMjBiM2MxNzMzMWYwYWE2L2Q1NDEzNzhhOTg2YmY2MzQ3OTJlOThhODMxNDUxOGQ1MDc0YmMwNjhjZTA5NDQyYmE3YTQxMmUwM2UzYzAzMjg%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=BJOPYgk991UQ69i2bG23EPhL8JpC-NfDxV3VZ983y1Lcen8jOtK5fCRwBTN2XO-eypRnp7XFIDOcpw4K3sP5eHTPRgbYJeb%7EBfaP4hZgf1uPt1sPqJKSY9-o6nf4CtDRuzfhkWWi98g3AG80pmcnFnQNo7efXboRTDaqQrH82SUPLywbos-hsR6PZmnnSuSJzApIv8Jlh0b40l1REl2EdJaNbc66KNVxbd2nG2prYX4v1N6WXNDEKN6QxhPFYh1vGnDGMK7pel6Kp3y4g-SRTy27uWEbxz8%7E7X4sZKb2ia93F95HS2I8e-rl7QAX6NKQryqWm9xO%7EHP09wWIQVk0WA__&Key-Pair-Id=KVTP0A1DKRTAX)

*An infographic on multimodality and why it is important to capture the overall sense of data through different modalities. The infographic is multimodal as well (image + text).*


Many times communication between 2 people gets really awkward in textual mode, slightly improves when voices are involved but greatly improves
when you are able to visualize body language and facial expressions as well. This has been studied in detail by the American Psychologist, Albert Mehrabian who stated this as 
the 7-38-55 rule of communication, the rule states:
"In communication, 7% of the overall meaning is conveyed through verbal mode (spoken words), 38% through voice and tone and 55% through body language and facial expressions."

![Funny Image + Text Meme example](https://cdn-lfs.huggingface.co/repos/bf/2a/bf2a50c0acddc20a4963bc4c9bfd57f4a0a887faf272014a120b3c17331f0aa6/37ae9115e29850f78c5a7e9c247b42ed124d10fdf6b6064ddaf4950897493a37?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27bigbang.jpg%3B+filename%3D%22bigbang.jpg%22%3B&response-content-type=image%2Fjpeg&Expires=1702912274&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMjkxMjI3NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iZi8yYS9iZjJhNTBjMGFjZGRjMjBhNDk2M2JjNGM5YmZkNTdmNGEwYTg4N2ZhZjI3MjAxNGExMjBiM2MxNzMzMWYwYWE2LzM3YWU5MTE1ZTI5ODUwZjc4YzVhN2U5YzI0N2I0MmVkMTI0ZDEwZmRmNmI2MDY0ZGRhZjQ5NTA4OTc0OTNhMzc%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=wj2nBCocSV9xyGcFn1zH-r4bkJWOQzmBp7nnKXpaBbBxDEw%7EuVc0Uvqlnljhi2Gplxz4hkVQzZPXq1nzDqXyuYfzWY4f7Z87micF-DDZznRCkj-TAmA5tzPSwLqH2N7mkS7Nv3C5n8US13ua18a%7Ebs7%7EwfYddSSMbRYIqY5vUEMYE5kLSdSlAh5vU3LmphpzD9%7ELF28NRlR%7EDt2CBZN4FP7AzVc6BhEPPaEMcEn10her0yQLuAJi2aBoAwF6qDHIImppcC5viSS4iLM3UXlZ3FJd6%7E13hO0Hms-VFTr80k1OE579lDWquvN5ehoMGo-cVvnmnqs-rL%7En5LnH4H6jcQ__&Key-Pair-Id=KVTP0A1DKRTAX)

To be more general, in the context of AI, 7% of the meaning conveyed is through textual modality, 38% through audio modality and 55% through vision modality.
Within the context of deep learning, we would refer each modality as a way data arrives to a deep learning model for processing and predictions. The most commonly used modalities in deep learning are: vision, audio and text. 
Other modalities can also be considered for specific usecases like LIDAR, EEG Data, eye tracking data etc.

Unimodal models and datasets are purely based on a single modality, and have been studied for long with many tasks and benchmarks but are limited in their capabilities. Relying on a single modality might not give us the complete picture, and combining more modalities will increase the information content and reduce the possibility of 
missing cues that might be in them. 
For the machines around us to be more intelligent, better at communicating with us and have enhanced interpretation and reasoning capabilities, it is important
to build applications and services around models and datasets that are multimodal in nature. Because, multimodality can give us a clearer and more accurate representation
of the world around us enabling us to develop applications that are closer to the real-world scenarios.

**Common combinations of modalities and real life examples:**
- Vision + Text : Infographics, Memes, Articles, Blogs.
- Vision + Audio: A Skype call with your friend, dyadic conversations.
- Vision + Audio + Text: Watching YouTube videos or movies with captions, social media content in general is multimodal. 
- Audio + Text: Voice notes, music files with lyrics

## Multimodal Datasets
A dataset consisting of multiple modalities is a multimodal dataset. Out of the common modality combinations let us see some examples:
- Vision + Text: [Visual Storytelling Dataset](https://visionandlanguage.net/VIST/), [Visual Question Answering Dataset](https://visualqa.org/download.html).
- Vision + Audio: [VGG-Sound Dataset](https://www.robots.ox.ac.uk/~vgg/data/vggsound/), [RAVDESS Dataset](https://zenodo.org/records/1188976), [Audio-Visual Identity Database (AVID)](https://www.avid.wiki/Main_Page).
- Vision + Audio + Text: [RECOLA Database](https://diuf.unifr.ch/main/diva/recola/), [IEMOCAP Dataset](https://sail.usc.edu/iemocap/).


Now let us see what kind of tasks can be performed using a multimodal dataset? There are many examples, but we will focus generally on tasks that contains the visual and textual 
A multimodal dataset will require a model which is able to process data from multiple modalities, such a model is a multimodal model.


## Multimodal Tasks and Models

Each modality has different tasks related to it, for example: vision downstream tasks contain image classification, image segmentation, object detection etc. and we would be using
models specifically designed for these tasks. So tasks and models go hand in hand.
If a task involves two or more modalities then it can be termed as a multimodal task. If we consider a task in terms of inputs and outputs,
a multimodal task can generally be thought of as a single input/output arrangement with two different modalities at input and output ends respectively.

HuggingFace supports a wide variety of multimodal tasks. Let us look into some of them.

**Some multimodal tasks supported by ü§ó and their variants:**
1. Vision + Text:
- [Visual Question Answering or VQA](https://huggingface.co/tasks/visual-question-answering): Aiding visually impaired persons, efficient image retrieval, video search, Video Question Answering, Document VQA.
- [Image to Text](https://huggingface.co/tasks/image-to-text): Image Captioning, Optical Character Recognition (OCR), Pix2Struct.
- [Text to Image](https://huggingface.co/tasks/text-to-image): Image Generation
- [Text to Video](https://huggingface.co/tasks/text-to-video): Text-to-video editing, Text-to-video search, Video Translation, Text-driven Video Prediction.

2. Audio + Text:  
- [Automatic Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition) (or Speech to Text): Virtual Speech Assistants, Caption Generation.
- [Text to Speech](https://huggingface.co/tasks/text-to-speech): Voice assistants, Announcement Systems.



<Tip>

üí°An amazing usecase of multimodal task is Multimodal Emotion Recognition (MER). The MER task involves
recognition of emotion from two or more modalities like audio+text, text+vision, audio+vision or vision+text+audio
As we discussed in the example, MER is more efficient than unimodal emotion recognition and gives clear insight into the 
emotion recognition task. Check out more on MER with this repository: https://github.com/EvelynFan/AWESOME-MER. 

</Tip>



![Multimodal model flow](https://cdn-lfs.huggingface.co/repos/bf/2a/bf2a50c0acddc20a4963bc4c9bfd57f4a0a887faf272014a120b3c17331f0aa6/632298d5f7b2ecd9b0f7bf4cf5843c85cd9e3004e46691782660279cb4d5b798?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Multimodal.jpg%3B+filename%3D%22Multimodal.jpg%22%3B&response-content-type=image%2Fjpeg&Expires=1702912253&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMjkxMjI1M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iZi8yYS9iZjJhNTBjMGFjZGRjMjBhNDk2M2JjNGM5YmZkNTdmNGEwYTg4N2ZhZjI3MjAxNGExMjBiM2MxNzMzMWYwYWE2LzYzMjI5OGQ1ZjdiMmVjZDliMGY3YmY0Y2Y1ODQzYzg1Y2Q5ZTMwMDRlNDY2OTE3ODI2NjAyNzljYjRkNWI3OTg%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=RAM-tWKhzF0gRKbt2zmlA7LsV3m0LChtVoS9oJZdxxZ07CoDH9eEW40889pbEGCtnex9uDY3EsoV2iw2Vtz0ThCbnQwSxR79rFipyzRI3qzrPAyjUOqGBF-Ntb3tifIjDKMyYU4C80gozZf8-Qe0fv%7EWMXn2W0AOXA23mziyIWQ%7ExWqZRiVqV5ZuHGV4G4DiehE1zEvLsve71J8iwmCE0LXdulbYADs%7EwqDmtSgxGSjGFDHxS8pFBrCuF-OWvpbH-1t2mTG1WOLgGQPjdEsdUDwRC%7EYBDNAczBqHLlVxfwUREs73-rQ0Q1pp31wJ4V01fEoNe3NXy9MEt1NcybYMLg__&Key-Pair-Id=KVTP0A1DKRTAX)

A multimodal model, is a model that can be used to performed multimodal tasks by processing data coming from multiple modalities at the same time.
These models combine the uniqueness and strengths of different modalities to make a complete representation of data enhancing the performance on multiple tasks.
Multimodal models are trained to integrate and process data from sources like images, videos, text, audio etc. The process of combination of these modalities begins
with multiple unimodal models, the outputs of  these unimodal models (encoded data) are then fused using a strategy by the fusion module. The strategy of fusion can be early fusion, 
late fusion or hybrid fusion. The overall task of the fusion module is to make a combined representation of the encoded data from the unimodal models.
Finally, the fused representation is taken up by a classification network to make predictions.


A detailed section on multimodal tasks and models with a focus on Vision and Text, will be discussed in the next chapter.


## An applicaion of multimodality: Multimodal Search üîéüì≤üíª


Internet search was the one key advantage Google had, but with the introduction of ChatGPT by OpenAI, Microsoft started out with
powering up their Bing search engine so that they can crush the competition. It was only restricted initially to LLMs, looking into large corpus of text data but the world around us, mainly social media content, web articles 
and all possible form of online content is largely multimodal. When we search about an image, the image pops up with a corresponding text to describe it. Won't it be super cool to have another powerful
multimodal model which involved both Vision and Text at the same time? This can revolutionize the search landscape hugely, and the core tech involved in this is multimodal learning.

We know that many companies also have a large database which is multimodal and mostly unstructured in nature. Multimodal models might help companies with internal search,
interactive documentation (chatbots), and many such usecases. This is another domain of EnterpriseAI where we leverage AI for organizational intelligence. 

Vision Language Models (VLMs) are models that can understand and process both Vision and Text modalities.
The joint understanding of both modalities lead VLMs to perform various tasks efficiently like Visual Question Answering, Text-to-image search etc. VLMs thus can serve as one of the best candidates for
multimodal search. So overall, VLMs should find some way to map text and image pairs to a joint embedding space where each such text-image pair is present as an embedding. Using these embeddings we can perform
various downstream tasks, and these embeddings can be used for search as well. The idea of such a joint space is that image and text embeddings that are similar in meaning will lie close together enabling us to 
do searches for images based on text (text-to-image search) or vice-a-versa.


<!-- Shall we add a demo or space here for ImageBind or just a plain GIF from the site? -->

<Tip>
üí°Meta released first multimodal AI model to bind information from 6 different modalities:  images and videos,
audio, text, depth, thermal, and inertial measurement units (IMUs). Check more here: https://imagebind.metademolab.com/
</Tip>

After going through the fundamentals for multimodality, let's now take a look into different multimodal tasks and models available in ü§ó and their applications via cool demos and spaces.