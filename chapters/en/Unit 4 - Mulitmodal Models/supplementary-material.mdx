# Supplemental reading and resources ðŸ¤—

We hope that you found the unit on Multimodal Models exciting. If you'd like to learn and explore in detail about Multimodal Learning and Models, here is a list of resources
for your reference:

1. [**Hugging Face Tasks**](https://huggingface.co/tasks) hosts a wide variety of tasks related to domains like Computer Vison, Audio, NLP, Multimodal Learning and Reinforcement Learning. The tasks contain demos, use-cases, models, datasets etc.
2. [**11-777 MMML**](https://cmu-multicomp-lab.github.io/mmml-course/fall2022/) course on Multimodal Machine Learning by CMU. You can find the video lectures [**here**](https://www.youtube.com/@LPMorency/playlists).
3. [**Blog on Multimodality and LLMs by Chip Huyen**](https://huyenchip.com/2023/10/10/multimodal.html) provides an comprehensive overview of multimodality, large multimodal models, systems like BLIP, CLIP etc.
4. [**Awesome Multimodal ML**](https://github.com/pliang279/awesome-multimodal-ml), a GitHub repository containing papers, courses, architectures, workshops, tutorials etc.
5. [**Awesome Multimodal Large Language Models**](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models), a GitHub repository containing papers and datasets related to multimodal LLMs. 
6. [**EE/CS 148, Caltech**](https://gkioxari.github.io/teaching/cs148/) course on Large Language and Vision Models.
7. [**16-892 CMU**](https://www.cs.cmu.edu/~deva/teach/16-892/index.html) seminar on Multimodal Foundation Models, Fall 2023. 
