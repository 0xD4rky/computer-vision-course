# Introduction
This section explains how stereo vision works and how it can be used to find the 3 dimensional structure of surrounding objects. Stereo vision means capturing 2 or more images of the same scene from different positions and/or viewpoints. Images could be captured by using 2 or more cameras, or by moving the same camera.

# Problem statement 
Let's understand the problem statement of finding the 3D structure of objects by understanding the geometry of image formation. As shown in figure 1, we have a point P in 3D with x,y,z coordinates. Point P gets projected to the camera's image plane via the pinhole. This can also be viewed as projecting a 3D point to a 2D image plane. 

Now, let's say we are given this 2D image and location of the pixel coordinates of point P in this image. We would like to find the 3D coordinates of point P. Is this possible ? Is point P unique or are there other 3D points which also map to the same pixel coordinates as point P ? Answer is all 3D points which lie on the line joining point P and the pinhole will map to the same pixel coordinates in the 2D image plane. 

For finding the 3D structure of objects, this is the problem we are trying to solve. For our problem statement we can view an object in 3D as a collection of 3D points. Finding the 3D coordinates of each of these points helps us determine the 3D structure of the object.

Figure 1: Image formation using single camera
![Figure 1: Image formation using single camera](https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/3d_stereo_vision_images/image_formation_single_camera.png?download=true )

# Solution 
Let's assume we are given the following information:

1. Single image of a scene point P
2. Pixel coordinates of point P in the image 
3. Position and orientation of the camera used to capture the image. For simplicity, we can also place a coordinate system xyz at the location of the pinhole, with z axis perpendicular to the image place and x,y axis parallel to the image plane like in figure 1.
4. Internal parameters of the camera such as focal length and location of principle point. Principle point is the point where the optical axis intersects the image plane. It's location in the image plane is usually denoted as (Ox,Oy).

Using above information, we can find a line in 3D which starts at the pixel coordinates of point P (i.e projection of point P in the image plane), passes through the pinhole and extends to infinity. From the geometry of image formation, we also know that point P must lie somewhere on this line. 

1. Initially (without an image) point P could have been present anywhere in the 3D space. 
2. Using a single image, we reduced possible locations of point P to a single line in 3D. 
3. Now think if we can further reduce possible locations to find precise location of point P on this 3D line ? 
4. Imagine moving the camera to a different position. Let the coordinate system remain fixed at the previous position. The 3D line we found also remains the same and point P still lies somewhere on this line.
5. From this new location of the camera, capture another image of the same scene point P. Again using pixel coordinates of point P in this new image and the new location of the camera pinhole, find the line in 3D on which point P must be lie. 
6. Now we have 2 lines in 3D and point P lies somewhere on both these lines. So, point P must lie on the intersection of these 2 lines. 

Given 2 lines in 3D, there are are three possibilities for their intersection:

1. Intersect at exactly 1 point 
2. Intersect at infinite number of points 
3. Do not intersect 

If both images (with original and new camera positions) contain point P, then we know that 3D lines must intersect at atleast one point which is point P. Also, we can imagine both lines intersecting at infinite number of points only if the 2 lines are collinear which is possible if the pinhole at the new camera position lies on the original 3D line. For all other positions and orientations of the new camera location, the 2 lines in 3D must intersect at exactly 1 point, which is where point P must lie. 

Therefore using 2 images of the same scene point P, known positions and orientations of the camera locations and known internal parameters of the camera, we can precisely find where point P lies in the 3D space.

# Simplified Solution 
Since there are many different positions and orientations for the camera locations which can be selected, we can select a location which makes the maths simpler, less complex and reduces computational processing when running on a computer or an embedded device. One configuration which is popular and generally used is shown in figure 2. We use 2 cameras in this configuration but it is equivalent to using a single camera for capturing 2 images from 2 different locations.

Figure 2: Image formation using 2 cameras 
![Figure 2: Image formation using 2 cameras](https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/3d_stereo_vision_images/image_formation_simple_stereo.jpg?download=true)

1. Origin of the coordinate system is placed at the pinhole of the first camera which is usually the left camera.
2. Z axis of the coordinate system is defined perpendicular to the image plane. 
3. X and Y axis of the coordinate system are defined parallel to the image plane.  
4. We also have X and Y directions in a 2D image. X is the horizontal direction and Y is the vertical direction. We will refer to these directions in the image plane as u and v respectively. Therefore, pixel coordinates of a point are defined using (u,v) values.  
5. X axis of the coordinate system is defined as the u direction / horizontal direction in the image plane.
6. Similarly Y axis of the coordinate system is defined as the v direction / vertical direction in the image plane. 
7. Second camera (more precisely pinhole of second camera) is placed at a distance b called baseline in the positive x direction to the right of the first camera. Therefore, x,y,z coordinates of pinhole of second camera are (b,0,0)
5. Image plane of the second camera is oriented parallel to the image plane of the first camera.  
6. u and v directions in the image plane of second/right camera are aligned with the u and v directions in the image plane of the first/left camera
7. Both left and right cameras are assumed to have the same intrinsic parameters like focal length and location of principle point

With the above configuration in place, we have the below equations which map a point in 3D to the image plane in 2D. 

1. Left camera
    1. $$u\_left = f\_x * \frac{x}{z} + O\_x$$
    2. $$v\_left = f\_y * \frac{y}{z} + O\_y$$ 
2. Right camera 
    1. $$u\_right = f\_x * \frac{x-b}{z} + O\_x$$
    2. $$v\_right = f\_y * \frac{y}{z} + O\_y$$ 

Different symbols used in above equations are defined below: 
* u_left, v_left refer to pixel coordinates of point P in the left image 
* u_right, v_right refer to pixel coordinates of point P in the right image 
* f_x refers to the focal length (in pixels) in x direction and f_y refers to the focal length (in pixels) in y direction. Actually, there is only 1 focal length for a camera which is the distance between the pinhole (/ optical center of the lens) to the image plane. However, pixels may be rectangular and not perfect squares, resulting in different fx and fy values when we represent f in terms of pixels.
* x,y,z are 3D coordinates of the point P (any unit like cm, feet, etc can be used)
* O_x and O_y refer to pixel coordinates of the principle point 
* b is called the baseline and refers to the distance between the left and right cameras. Same units are used for both b and x,y,z coordinates (any unit like cm, feet, etc can be used)

We have 4 equations above and 3 unknowns - x, y and z coordinates of a 3D point P. Intrinsic camera parameters - focal lengths and principle point are assumed to be known. Equations 1.2 and 2.2 indicate that the v coordinate value in the left and right images is the same. 

3. v_left = v_right 

Using equations 1.1, 1.2 and 2.1 we can derive the x,y,z coordinates of point P. 

1. $$x = \frac{b * (u\_left - O\_x)}{u\_left - u\_right}$$
2. $$y = \frac{b * f\_x * (v\_left - O\_y)}{ f\_y * (u\_left - u\_right)}$$ 
3. $$z = \frac{b * f\_x}{u\_left - u\_right}$$

Note that x and y values above are with respect to the left camera since origin of the coordinate system is aligned with the left camera. Above equations show that we can find 3D coordinates of a point P using it's 2 images captured from 2 different camera locations. z value is also referred to as the depth value. Using this technique, we can find the depth values for different pixels within an image along with their real world x and y coordinates. We can also find real world distances between different points in an image. 

# Demo 
## Setup 
We'll work through an example, capture some images, perform some calculations to find out if our above assumptions and maths works out! For capturing the images we'll use a hardware device known as OAK-D Lite (OAK stands for OpenCV AI Kit). This device has 3 cameras - left and right mono (black and white) cameras and center colour camera. We'll use the left and right mono cameras for our experiement. A regular smartphone camera could also be used but using an OAK-D lite has some advantages listed below.

* Intrinsic camera parameters like focal length and location of principle point are known since the device comes pre-calibrated and these parameters can be read from the device using it's python API. For a smartphone camera, intrinsic parameters need to be determined and could be found by performing camera calibration or sometimes present in the metadata of the image captured using the smartphone. 
* One of our main assumptions above is that the position and orientation of the left and right cameras are known. Using a smartphone camera, it may be difficult to precisely determine this information or an additional calibration may be required.  On the other hand, for an OAK-D Lite device position and orientation of the left and right cameras is fixed, known, pre-calibrated and very similar to the geometry of the simplified solution mentioned above. Although some post-processing/image rectification detailed below on the raw images is still required.

## Original Left and Right Images
TBD, include baseline distance, vertical positions and why discrepenacy , follows simplied geometry


## Rectified Left and Right Images 
TBD, explain the rectification process and link to source code, explain the stacking (aligned now) and overlapping with disparity meaning and disparity with depth

## Annotated Left and Right Rectified Images
TBD

## 3D Coordinate Calculations  
TBD 

| point    |   u_left  |   v_left  |   u_right  |   v_right  |   depth/z(cm)  |   x_wrt_left(cm)  |   y_wrt_left(cm)  |
|:--------:|:---------:|:---------:|:----------:|:----------:|:--------------:|:-----------------:|:-----------------:|
| pt1     |      138 |      219 |       102 |       219 |         94.36 |           -33.51 |            -5.53 |
| pt2     |      264 |      216 |       234 |       217 |        113.23 |            -8.72 |            -7.38 |
| pt3     |      137 |      320 |       101 |       321 |         94.36 |           -33.72 |            15.52 |
| pt4     |      263 |      303 |       233 |       302 |        113.23 |            -8.97 |            14.37 |
| pt5     |      307 |      211 |       280 |       211 |        125.81 |             2.26 |            -9.59 |
| pt6     |      367 |      212 |       339 |       212 |        121.32 |            18.25 |            -8.98 |
| pt7     |      305 |      298 |       278 |       298 |        125.81 |             1.71 |            14.58 |
| pt8     |      365 |      299 |       338 |       299 |        125.81 |            18.37 |            14.86 |
| pt9     |      466 |      225 |       415 |       225 |         66.61 |            24.58 |            -3.02 |
| pt10    |      581 |      225 |       530 |       226 |         66.61 |            41.49 |            -3.02 |
| pt11    |      464 |      387 |       413 |       388 |         66.61 |            24.29 |            20.81 |
| pt12    |      579 |      388 |       528 |       390 |         66.61 |            41.2  |            20.95 |


## Dimension Calculations and Accuracy 
TBD 

| dimension    |   measured(cm)  |   actual(cm)  |       % error       |
|:------------:|:---------------:|:-------------:|:-------------------:|
| d1(1-2)     |           31.2 |         31.2 |               0    |
| d2(1-3)     |           21.1 |         21.3 |               0.94 |
| d3(5-6)     |           16.6 |         16.7 |               0.6  |
| d4(5-7)     |           24.2 |         24   |               0.83 |
| d5(9-10)    |           16.9 |         16.7 |               1.2  |
| d6(9-11)    |           23.8 |         24   |               0.83 |

## Observations 
TBD 

# Summary and Key Insights 
TBD

# References 
TBD