# DEtection TRansformer (DETR)
## Overview of architecture
yolo: https://arxiv.org/abs/1506.02640
yolov3: https://arxiv.org/abs/1804.02767
A traditional object detection model like YOLOv3 consists of feature extraction backbones, followed by a detector that uses hand-crafted features and post-processing to detect objects. Hand-crafted features like anchor box priors require initial guesses of object locations and shapes, which influence downstream training. Post-processing steps like non-max suppression are then used to remove overlapping bounding boxes, which require careful selection of its filtering heuristics. 
DEtection TRansformer, DETR for short, simplifies the detector head by using an encoder-decoder transformer after the feature extraction backbone to directly predict bounding boxes in parallel. 
The model architecture of DETR begins with a CNN backbone, similar to other image-based networks, the output of which is processed and fed into a transformer encoder, resulting in N embeddings. The encoder embeddings are added to learned positional embeddings and used in a transformer decoder, generating another N embeddings. As a final step, each of the N embeddings are put through individual feed forward layers to predict the width, height, coordinates of the bounding box, as well as the object class (or whether there is an object). 

## Advantages

## Key Features

### Encoder-Decoder
As with other transformers, it expects the output to be a sequence. Thus, the feature map of size DxHxW is flattened to DxHW.
Since transformers are permutation invariant, positional embeddings are added to both the encoder and decoder to remind the model where on the image the embeddings come from. In the encoder, fixed positional encodings are used, while in the decoder, learned positional encodings are used. 

### Set-based Global Loss Function
In YOLO, the loss function is composed of bounding box, objectness and class loss. The loss is calculated over multiple bounding boxes per each grid cell, which is a fixed number. On the other hand, in DETR, the architecture is expected to generate unique bounding boxes in a permutation-invariant manner (ie. the order of the detections do not matter in the output, and the bounding boxes must vary and cannot all be the same). Thus, matching is required to assess how good the predictions are.

#### Bipartite Matching
Bipartite matching is done to find one to one matches with the lowest cost. The cost function is defined to be the similarity between ground truth and predicted bounding boxes, as well as classes. Hungarian algorithm is used to optimally find the matching pairs. With these, class and bounding box loss are calculated for all matched pairs.  


## Evolution of DETR
### Conditional DETR
https://arxiv.org/abs/2108.06152

### Deformable DETR
https://arxiv.org/abs/2010.04159

## Implementation 
The implementation of DETR from the paper is shown below:
```python
import torch
from torch import nn
from torchvision.models import resnet50

class DETR(nn.Module):

    def __init__(self, num_classes, hidden_dim, nheads,
         num_encoder_layers, num_decoder_layers):
        super().__init__()
        self.backbone = nn.Sequential(*list(resnet50(pretrained=True).children())[:-2])
        self.conv = nn.Conv2d(2048, hidden_dim, 1)
        self.transformer = nn.Transformer(hidden_dim, nheads,
        num_encoder_layers, num_decoder_layers)
        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)
        self.linear_bbox = nn.Linear(hidden_dim, 4)
        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))
        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))
        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))

    def forward(self, inputs):
        x = self.backbone(inputs)
        h = self.conv(x)
        H, W = h.shape[-2:]
        pos = torch.cat([
            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),
            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),
        ], dim=-1).flatten(0, 1).unsqueeze(1)
        h = self.transformer(pos + h.flatten(2).permute(2, 0, 1),
        self.query_pos.unsqueeze(1))
        return self.linear_class(h), self.linear_bbox(h).sigmoid()
```
Going line by line in the `forward` function: <br/>
#### Backbone
The input image is first put through a resnet backbone and then a convolution layer, which reduces the dimension to the `hidden_dim`
```python
x = self.backbone(inputs)
h = self.conv(x)
```
they are declared in the `__init__` function
```python
self.backbone = nn.Sequential(*list(resnet50(pretrained=True).children())[:-2])
self.conv = nn.Conv2d(2048, hidden_dim, 1)
```
#### Positional Embeddings
While in the paper fixed and trained embeddings are used in the encoder and decoder respectively, the authors used trained embeddings for both in the implementation for simplicity.    
```python
pos = torch.cat([
    self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),
    self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),
], dim=-1).flatten(0, 1).unsqueeze(1)
```
They are declared here as `nn.Parameter`. The row and column embeddings combined denote the locations in the image.
```python
self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))
self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))
```
#### Resize
Before going into the transformer, the features with size `(1, hidden_dim, H, W)` are reshaped to `(hidden_dim, 1, H*W)`. This makes them a sequential input for the transformer
```python
h.flatten(2).permute(2, 0, 1)
```
#### Transformer
The `nn.Transformer` function takes in the first parameter as the input to the encoder, and the second parameter as the input of the encoder. As you can see, the encoder takes in the resized features added to the positional embeddings, while the decoder takes in `query_pos`, which is the decoder positional embedding.
```python
h = self.transformer(pos + h.flatten(2).permute(2, 0, 1),self.query_pos.unsqueeze(1))
```
#### Feed-Forward Network
In the end, the outputs, which is a tensor of size `(query_pos_dim, 1, hidden_dim)`, is fed through two linear layers. 
```python
return self.linear_class(h), self.linear_bbox(h).sigmoid()
```
The first of which predicts the class. An additional class is added for the `No Object` class
```python
self.linear_class = nn.Linear(hidden_dim, num_classes + 1)
```
The second linear layer predicts the bounding box with an output size 4 for the xy coordinates, height and width.
```python
self.linear_bbox = nn.Linear(hidden_dim, 4)
```