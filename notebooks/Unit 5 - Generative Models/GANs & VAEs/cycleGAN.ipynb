{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will implement [CycleGAN](https://arxiv.org/abs/1703.10593) by Zhu et al. 2017.\n",
    "\n",
    "You will be training a model that can convert horses into zebras, and vice versa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Step 1: We will start by importing some necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import torch.nn.functional as F\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Below code downloads the zebra2horses dataset and place it in same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# URL of the dataset\n",
    "url = 'https://efrosgans.eecs.berkeley.edu/cyclegan/datasets/horse2zebra.zip'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Ensure the request is successful\n",
    "if response.status_code == 200:\n",
    "    z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "    z.extractall()  # You can specify a different directory inside the parentheses\n",
    "    print(\"Download and extraction complete.\")\n",
    "else:\n",
    "    print(\"Error downloading the file. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Define our Custom dataset class function and transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by https://github.com/aitorzip/PyTorch-CycleGAN/blob/master/datasets.py\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, mode='train'):\n",
    "        self.transform = transform\n",
    "        self.files_A = sorted(glob.glob(os.path.join(root, '%sA' % mode) + '/*.*'))\n",
    "        self.files_B = sorted(glob.glob(os.path.join(root, '%sB' % mode) + '/*.*'))\n",
    "        if len(self.files_A) > len(self.files_B):\n",
    "            self.files_A, self.files_B = self.files_B, self.files_A\n",
    "        self.new_perm()\n",
    "        assert len(self.files_A) > 0, \"Make sure you downloaded the horse2zebra images!\"\n",
    "\n",
    "    def new_perm(self):\n",
    "        self.randperm = torch.randperm(len(self.files_B))[:len(self.files_A)]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]))\n",
    "        item_B = self.transform(Image.open(self.files_B[self.randperm[index]]))\n",
    "        if item_A.shape[0] != 3: \n",
    "            item_A = item_A.repeat(3, 1, 1)\n",
    "        if item_B.shape[0] != 3: \n",
    "            item_B = item_B.repeat(3, 1, 1)\n",
    "        if index == len(self) - 1:\n",
    "            self.new_perm()\n",
    "        # Old versions of PyTorch didn't support normalization for different-channeled images\n",
    "        return (item_A - 0.5) * 2, (item_B - 0.5) * 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.files_A), len(self.files_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_shape = 286\n",
    "target_shape = 256\n",
    "\n",
    "# basic transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(load_shape),\n",
    "    transforms.RandomCrop(target_shape),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "import torchvision\n",
    "dataset = ImageDataset(\"horse2zebra\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_shifted = image_tensor\n",
    "    image_unflat = image_shifted.detach().cpu().view(-1, *size)\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CycleGAN](https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/cycleGAN1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "CycleGAN generator is composed of encoding blocks, residual blocks, and then decoding blocks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Block:\n",
    "These blocks add the output of convolutional layers to the input, enabling minimal changes to the image. This method acts as a skip connection, facilitating deeper networks by mitigating vanishing gradients and allowing the learning of more complex features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    '''\n",
    "    Defines a residual block for a neural network.\n",
    "\n",
    "    This block consists of two convolutional layers with instance normalization,\n",
    "    followed by an activation function. The input is added to the output of\n",
    "    these layers, forming the final output of the block.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_channels):\n",
    "        '''\n",
    "        Initializes the ResidualBlock.\n",
    "\n",
    "        Parameters:\n",
    "        input_channels (int): Number of channels in the input tensor.\n",
    "        '''\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        # First convolutional layer with reflective padding\n",
    "        self.conv1 = nn.Conv2d(input_channels, input_channels, kernel_size=3, padding=1, padding_mode=\"reflect\")\n",
    "\n",
    "        # Second convolutional layer with reflective padding\n",
    "        self.conv2 = nn.Conv2d(input_channels, input_channels, kernel_size=3, padding=1, padding_mode=\"reflect\")\n",
    "\n",
    "        # Instance normalization layer\n",
    "        self.instancenorm = nn.InstanceNorm2d(input_channels)\n",
    "\n",
    "        # Activation function (ReLU)\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Defines the forward pass of the ResidualBlock.\n",
    "\n",
    "        Parameters:\n",
    "        x (Tensor): Input tensor to the residual block.\n",
    "\n",
    "        Returns:\n",
    "        Tensor: Output tensor after applying the residual block operations.\n",
    "        '''\n",
    "        # Save the original input for use in the skip connection\n",
    "        original_tensor_image = x.clone()\n",
    "\n",
    "        # First convolutional operation\n",
    "        x = self.conv1(x)\n",
    "        x = self.instancenorm(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        # Second convolutional operation\n",
    "        x = self.conv2(x)\n",
    "        x = self.instancenorm(x)\n",
    "\n",
    "        # Add the original tensor to the output (skip connection)\n",
    "        return original_tensor_image + x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and Decoder Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    '''\n",
    "    EncoderBlock Class:\n",
    "    A class used to create an encoder block in a neural network, which consists of a convolutional layer,\n",
    "    an optional instance normalization layer, and an activation function.\n",
    "    Attributes:\n",
    "        conv1: Conv2d layer to reduce spatial dimensions and increase channels.\n",
    "        instancenorm: InstanceNorm2d layer for normalization (optional).\n",
    "        activation: Activation function (ReLU or LeakyReLU).\n",
    "        use_bn: Boolean, whether to use batch normalization.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_channels, use_bn=True, kernel_size=3, activation='relu'):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        # Create a convolutional layer with stride 2 for downsampling\n",
    "        self.conv1 = nn.Conv2d(input_channels, input_channels * 2, kernel_size=kernel_size, padding=1, stride=2, padding_mode='reflect')\n",
    "        # Choose activation function based on the input argument\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        else:\n",
    "            self.activation = nn.LeakyReLU(0.2)\n",
    "        # Add instance normalization if use_bn is True\n",
    "        if use_bn:\n",
    "            self.instancenorm = nn.InstanceNorm2d(input_channels * 2)\n",
    "        \n",
    "        self.use_bn = use_bn\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Function for the forward pass of EncoderBlock:\n",
    "        Applies a convolution, optional normalization, and activation to the input tensor.\n",
    "        '''\n",
    "        x = self.conv1(x)\n",
    "        if self.use_bn:\n",
    "            x = self.instancenorm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    ''' \n",
    "    DecoderBlock Class:\n",
    "    A class used to create a decoder block in a neural network, which consists of a transposed convolutional layer\n",
    "    (for upsampling), an optional instance normalization layer, and a ReLU activation function.\n",
    "    Attributes:\n",
    "        conv1: ConvTranspose2d layer for upsampling and reducing channels.\n",
    "        instancenorm: InstanceNorm2d layer for normalization (optional).\n",
    "        use_bn: Boolean, whether to use batch normalization.\n",
    "        activation: ReLU activation function.\n",
    "    ''' \n",
    "\n",
    "    def __init__(self, input_channels, use_bn=True):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        # Transposed convolutional layer for upsampling\n",
    "        self.conv1 = nn.ConvTranspose2d(input_channels, input_channels // 2, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        # Add instance normalization if use_bn is True\n",
    "        if use_bn:\n",
    "            self.instancenorm = nn.InstanceNorm2d(input_channels // 2)\n",
    "        self.use_bn = use_bn\n",
    "        # ReLU activation function\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        ''' \n",
    "        Function for the forward pass of DecoderBlock:\n",
    "        Applies a transposed convolution, optional normalization, and activation to the input tensor.\n",
    "        ''' \n",
    "        x = self.conv1(x)\n",
    "        if self.use_bn:\n",
    "            x = self.instancenorm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class FeatureMapBlock(nn.Module):\n",
    "    ''' \n",
    "    FeatureMapBlock Class:\n",
    "    A class for a basic convolutional block used to modify the number of feature maps\n",
    "    without changing the spatial dimensions of the input.\n",
    "    Attributes:\n",
    "        conv: Conv2d layer with a large kernel size to capture features without reducing spatial size.\n",
    "    ''' \n",
    "\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(FeatureMapBlock, self).__init__()\n",
    "        # Convolutional layer with a larger kernel size for feature mapping\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels, kernel_size=7, padding=3, padding_mode='reflect')\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Function for the forward pass of FeatureMapBlock:\n",
    "        Applies a convolution to the input tensor to change the number of feature maps.\n",
    "        '''\n",
    "        x = self.conv(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CycleGAN Generator\n",
    "Now, let's put all the blocks together to create your CycleGAN Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    ''' \n",
    "    ''' \n",
    "\n",
    "    def __init__(self, input_channels, output_channels, hiddne_channels=64):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.upfeature = FeatureMapBlock(input_channels, hiddne_channels)\n",
    "        self.encoder1 = EncoderBlock(hiddne_channels)\n",
    "        self.encoder2 = EncoderBlock(hiddne_channels * 2)\n",
    "        \n",
    "        res_mult = 4\n",
    "        self.res0 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.res1 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.res2 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.res3 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.res4 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.res5 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.res6 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.res7 = ResidualBlock(hidden_channels * res_mult)\n",
    "        self.res8 = ResidualBlock(hidden_channels * res_mult)\n",
    "        \n",
    "        self.decoder1 = DecoderBlock(hidden_channels * 4)\n",
    "        self.decoder2 = DecoderBlock(hidden_channels * 2)\n",
    "        \n",
    "        self.downfeature = FeatureMapBlock(hidden_channels, output_channels)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        '''\n",
    "        x0 = self.upfeature(x)\n",
    "        x1 = self.encoder1(x0)\n",
    "        x2 = self.encoder2(x1)\n",
    "        x3 = self.res0(x2)\n",
    "        x4 = self.res1(x3)\n",
    "        x5 = self.res2(x4)\n",
    "        x6 = self.res3(x5)\n",
    "        x7 = self.res4(x6)\n",
    "        x8 = self.res5(x7)\n",
    "        x9 = self.res6(x8)\n",
    "        x10 = self.res7(x9)\n",
    "        x11 = self.res8(x10)\n",
    "        x12 = self.decoder1(x11)\n",
    "        x13 = self.decoder2(x12)\n",
    "        xn = self.downfeature(x13)\n",
    "        return self.tanh(xn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class:\n",
    "    Defines a generator network for a generative model, such as a GAN.\n",
    "    The generator network consists of an initial feature map block, several encoder blocks, \n",
    "    a series of residual blocks, followed by decoder blocks and a final feature map block.\n",
    "\n",
    "    Attributes:\n",
    "        input_channels (int): Number of input channels (e.g., for an RGB image, it would be 3).\n",
    "        output_channels (int): Number of output channels (e.g., for generating RGB images, it would be 3).\n",
    "        hidden_channels (int): Base number of channels used in the network, which scales up in deeper layers.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_channels, output_channels, hidden_channels=64):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Initial feature mapping to scale up the number of channels\n",
    "        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)\n",
    "\n",
    "        # Encoder blocks - progressively increase channel depth, reducing spatial dimensions\n",
    "        self.encoder1 = EncoderBlock(hidden_channels)\n",
    "        self.encoder2 = EncoderBlock(hidden_channels * 2)\n",
    "        \n",
    "        # Set the multiplier for the number of channels in residual blocks\n",
    "        res_mult = 4\n",
    "\n",
    "        # Series of residual blocks - these help in preserving high-level features through the network\n",
    "        self.res_blocks = [ResidualBlock(hidden_channels * res_mult) for _ in range(9)]\n",
    "\n",
    "        # Decoder blocks - increase spatial dimensions and decrease channel depth\n",
    "        self.decoder1 = DecoderBlock(hidden_channels * 4)\n",
    "        self.decoder2 = DecoderBlock(hidden_channels * 2)\n",
    "        \n",
    "        # Final feature mapping to get the desired output channels\n",
    "        self.downfeature = FeatureMapBlock(hidden_channels, output_channels)\n",
    "\n",
    "        # Activation function to normalize output (e.g., to scale the output to a range of -1 to 1)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Defines the forward pass of the generator.\n",
    "        '''\n",
    "        x0 = self.upfeature(x)\n",
    "        x1 = self.encoder1(x0)\n",
    "        x2 = self.encoder2(x1)\n",
    "\n",
    "        # Pass through all residual blocks\n",
    "        res_output = x2\n",
    "        for res_block in self.res_blocks:\n",
    "            res_output = res_block(res_output)\n",
    "\n",
    "        x12 = self.decoder1(res_output)\n",
    "        x13 = self.decoder2(x12)\n",
    "\n",
    "        # Apply the final feature mapping and activation\n",
    "        xn = self.downfeature(x13)\n",
    "        return self.tanh(xn)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Block\n",
    "Now we will define the discriminator block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Discriminator Class:\n",
    "    Defines a discriminator network for a generative model, such as a GAN.\n",
    "    The discriminator's role is to distinguish between real and fake (generated) data.\n",
    "\n",
    "    Attributes:\n",
    "        input_channels (int): Number of input channels (e.g., for an RGB image, it would be 3).\n",
    "        hidden_channels (int): Base number of channels used in the network, which scales up in deeper layers.\n",
    "    '''\n",
    "    def __init__(self, input_channels, hidden_channels=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # Initial feature mapping to scale up the number of channels\n",
    "        self.upfeature = FeatureMapBlock(input_channels, hidden_channels)\n",
    "\n",
    "        # Contracting blocks (Encoder blocks) - reduce spatial dimensions and increase channel depth\n",
    "        # Using LeakyReLU activation (lrelu) for each contracting block\n",
    "        self.contract1 = EncoderBlock(hidden_channels, use_bn=False, kernel_size=4, activation='lrelu')\n",
    "        self.contract2 = EncoderBlock(hidden_channels * 2, kernel_size=4, activation='lrelu')\n",
    "        self.contract3 = EncoderBlock(hidden_channels * 4, kernel_size=4, activation='lrelu')\n",
    "\n",
    "        # Final convolution to output a single channel (binary classification for real or fake)\n",
    "        self.final = nn.Conv2d(hidden_channels * 8, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Defines the forward pass of the discriminator.\n",
    "        '''\n",
    "        # Sequentially apply feature mapping and contracting blocks\n",
    "        x0 = self.upfeature(x)\n",
    "        x1 = self.contract1(x0)\n",
    "        x2 = self.contract2(x1)\n",
    "        x3 = self.contract3(x2)\n",
    "\n",
    "        # Apply the final convolution\n",
    "        xn = self.final(x3)\n",
    "        return xn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Preperation\n",
    "\n",
    "Now you can put everything together for training! You will start by defining your parameters:\n",
    "\n",
    "  *   adv_criterion: an adversarial loss function to keep track of how well the GAN is fooling the discriminator and how well the discriminator is catching the GAN\n",
    "  *   recon_criterion: a loss function that rewards similar images to the ground truth, which \"reconstruct\" the image\n",
    "  *   n_epochs: the number of times you iterate through the entire dataset when training\n",
    "  *   dim_A: the number of channels of the images in pile A\n",
    "  *   dim_B: the number of channels of the images in pile B (note that in the visualization this is currently treated as equivalent to dim_A)\n",
    "  *   display_step: how often to display/visualize the images\n",
    "  *   batch_size: the number of images per forward/backward pass\n",
    "  *   lr: the learning rate\n",
    "  *   target_shape: the size of the input and output images (in pixels)\n",
    "  *   load_shape: the size for the dataset to load the images at before randomly cropping them to target_shape as a simple data augmentation\n",
    "  *   device: the device type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_criterion = nn.MSELoss() \n",
    "recon_criterion = nn.L1Loss() \n",
    "device = 'cuda'\n",
    "n_epochs = 20\n",
    "dim_A = 3\n",
    "dim_B = 3\n",
    "display_step = 200\n",
    "batch_size = 1\n",
    "lr = 0.0002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can initialize your generators and discriminators, as well as their optimizers. For CycleGAN, you will have two generators and two discriminators since there are two GANs:\n",
    "\n",
    "*   Generator for horse to zebra (`gen_AB`)\n",
    "*   Generator for zebra to horse (`gen_BA`)\n",
    "*   Discriminator for horse (`disc_A`)\n",
    "*   Discriminator for zebra (`disc_B`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_AB = Generator(dim_A, dim_B).to(device)\n",
    "gen_BA = Generator(dim_B, dim_A).to(device)\n",
    "gen_opt = torch.optim.Adam(list(gen_AB.parameters()) + list(gen_BA.parameters()), lr=lr, betas=(0.5, 0.999))\n",
    "disc_A = Discriminator(dim_A).to(device)\n",
    "disc_A_opt = torch.optim.Adam(disc_A.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "disc_B = Discriminator(dim_B).to(device)\n",
    "disc_B_opt = torch.optim.Adam(disc_B.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Training the model from scratch\n",
    "gen_AB = gen_AB.apply(weights_init)\n",
    "gen_BA = gen_BA.apply(weights_init)\n",
    "disc_A = disc_A.apply(weights_init)\n",
    "disc_B = disc_B.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CycleGAN Lossses\n",
    "\n",
    "Now We will define all the losses required for the CycleGAN:\n",
    "\n",
    "*   Discriminator Loss\n",
    "*   Generator Loss\n",
    "*   Identity Loss\n",
    "*   Cycle Consistency Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disc_loss(real_X, fake_X, disc_X, adv_criterion):\n",
    "    '''\n",
    "    Calculate the discriminator's loss.\n",
    "    Arguments:\n",
    "    - real_X: Real images from the dataset.\n",
    "    - fake_X: Fake images generated by the generator.\n",
    "    - disc_X: The discriminator network.\n",
    "    - adv_criterion: The adversarial loss function (usually binary cross-entropy).\n",
    "\n",
    "    Returns:\n",
    "    - disc_loss: The calculated loss for the discriminator.\n",
    "    '''\n",
    "\n",
    "    # Pass fake images through discriminator and calculate loss against zeros\n",
    "    disc_fake = disc_X(fake_X)\n",
    "    # The discriminator should output close to 0 for fake images\n",
    "    disc_fake_loss = adv_criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "\n",
    "    # Pass real images through discriminator and calculate loss against ones\n",
    "    disc_real = disc_X(real_X)\n",
    "    # The discriminator should output close to 1 for real images\n",
    "    disc_real_loss = adv_criterion(disc_real, torch.ones_like(disc_real))\n",
    "\n",
    "    # Average the fake and real loss\n",
    "    disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "\n",
    "    return disc_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen_adversarial_loss(real_X, disc_Y, gen_XY, adv_criterion):\n",
    "    '''\n",
    "    Calculate the generator's adversarial loss.\n",
    "    Arguments:\n",
    "    - real_X: Real images from domain X.\n",
    "    - disc_Y: The discriminator for domain Y.\n",
    "    - gen_XY: The generator that converts images from domain X to domain Y.\n",
    "    - adv_criterion: The adversarial loss function (usually binary cross-entropy).\n",
    "\n",
    "    Returns:\n",
    "    - adversarial_loss: The calculated adversarial loss for the generator.\n",
    "    - fake_Y: The generated images, transformed from domain X to domain Y.\n",
    "    '''\n",
    "\n",
    "    # Generate fake images in domain Y using the generator\n",
    "    fake_Y = gen_XY(real_X)\n",
    "\n",
    "    # Pass these fake images through the discriminator for domain Y\n",
    "    fake_disc_Y = disc_Y(fake_Y)\n",
    "\n",
    "    # Calculate the loss against ones\n",
    "    # The generator wants to fool the discriminator, so it aims for discriminator outputs close to 1\n",
    "    adversarial_loss = adv_criterion(fake_disc_Y, torch.ones_like(fake_disc_Y))\n",
    "\n",
    "    # Return the adversarial loss and the generated images\n",
    "    return adversarial_loss, fake_Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identity Loss\n",
    "\n",
    "The identity loss in CycleGAN measures how much a generator changes an image from the target domain when it should ideally output the same image unchanged. For example, passing a horse image to a zebra-to-horse generator should result in the same horse image, as the generator's task is unnecessary. This loss helps maintain image characteristics like color, especially in transformations where the input and target domains are similar, such as photos to paintings. Encouraging this identity mapping can improve the model's performance in specific applications by ensuring it doesn't apply unnecessary transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_identity_loss(real_X, gen_YX, identity_criterion):\n",
    "    '''\n",
    "    Calculates the identity loss for a generator.\n",
    "\n",
    "    This function computes the identity loss, which measures how much the generator \n",
    "    gen_YX alters an input from its target domain (real_X). The ideal behavior for \n",
    "    the generator is to leave the input unchanged if it already belongs to the target domain.\n",
    "\n",
    "    Parameters:\n",
    "    real_X (Tensor): A batch of real images from the target domain.\n",
    "    gen_YX (Module): The generator model that should ideally perform no transformation \n",
    "                     on the input images from the target domain.\n",
    "    identity_criterion (Loss Function): The loss function used to compute the identity loss.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[Tensor, Tensor]: A tuple containing the identity loss and the generated images \n",
    "                           (which ideally should be identical to the input images).\n",
    "    '''\n",
    "\n",
    "    # Pass the real images from the target domain through the generator\n",
    "    identity_X = gen_YX(real_X)\n",
    "\n",
    "    # Calculate the identity loss, which is the difference between the generator's output\n",
    "    # and the original images. The loss is lower when the generator alters the images less.\n",
    "    identity_loss = identity_criterion(real_X, identity_X)\n",
    "    \n",
    "    return identity_loss, identity_X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cycle Consistency Loss\n",
    "The cycle consistency loss in CycleGAN ensures that an image, when passed through a generator to another domain and then back to its original domain using a reverse generator, remains the same as the original. This loss encourages information preservation through the cycle of transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cycle_consistency_loss(real_X, fake_Y, gen_YX, cycle_criterion):\n",
    "    '''\n",
    "    Calculate the cycle consistency loss for a pair of images.\n",
    "\n",
    "    Parameters:\n",
    "    real_X (Tensor): The original images from domain X.\n",
    "    fake_Y (Tensor): The transformed images, originally from domain X, now in domain Y.\n",
    "    gen_YX (Module): The generator model that transforms images from domain Y back to domain X.\n",
    "    cycle_criterion (Loss Function): The criterion (loss function) used to evaluate cycle consistency.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Contains the cycle loss (a scalar Tensor) and the cycled images (cycle_X).\n",
    "    '''\n",
    "\n",
    "    # Transform the fake images from domain Y back to domain X\n",
    "    cycle_X = gen_YX(fake_Y)\n",
    "\n",
    "    # Calculate the cycle consistency loss (how close is cycle_X to the original real_X)\n",
    "    cycle_loss = cycle_criterion(real_X, cycle_X)\n",
    "\n",
    "    return cycle_loss, cycle_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen_loss(real_A, real_B, gen_AB, gen_BA, disc_A, disc_B, adv_criterion, identity_criterion, cycle_criterion, lambda_identity=0.1, lambda_cycle=10):\n",
    "    '''\n",
    "    Return the loss of the generator given inputs.\n",
    "    Parameters:\n",
    "        real_A: the real images from pile A\n",
    "        real_B: the real images from pile B\n",
    "        gen_AB: the generator for class A to B; takes images and returns the images \n",
    "            transformed to class B\n",
    "        gen_BA: the generator for class B to A; takes images and returns the images \n",
    "            transformed to class A\n",
    "        disc_A: the discriminator for class A; takes images and returns real/fake class A\n",
    "            prediction matrices\n",
    "        disc_B: the discriminator for class B; takes images and returns real/fake class B\n",
    "            prediction matrices\n",
    "        adv_criterion: the adversarial loss function; takes the discriminator \n",
    "            predictions and the true labels and returns a adversarial \n",
    "            loss (which you aim to minimize)\n",
    "        identity_criterion: the reconstruction loss function used for identity loss\n",
    "            and cycle consistency loss; takes two sets of images and returns\n",
    "            their pixel differences (which you aim to minimize)\n",
    "        cycle_criterion: the cycle consistency loss function; takes the real images from X and\n",
    "            those images put through a X->Y generator and then Y->X generator\n",
    "            and returns the cycle consistency loss (which you aim to minimize).\n",
    "            Note that in practice, cycle_criterion == identity_criterion == L1 loss\n",
    "        lambda_identity: the weight of the identity loss\n",
    "        lambda_cycle: the weight of the cycle-consistency loss\n",
    "    '''\n",
    "\n",
    "    # Adversarial Loss -- get_gen_adversarial_loss(real_X, disc_Y, gen_XY, adv_criterion)\n",
    "    adversarial_loss_AB, fake_B =  get_gen_adversarial_loss(real_A,disc_B,gen_AB,adv_criterion)\n",
    "    adversarial_loss_BA, fake_A =  get_gen_adversarial_loss(real_B,disc_A,gen_BA,adv_criterion)\n",
    "    adversarial_loss = adversarial_loss_AB + adversarial_loss_BA\n",
    "    # Identity Loss -- get_identity_loss(real_X, gen_YX, identity_criterion)\n",
    "    identity_loss_A, _ = get_identity_loss(real_A, gen_BA, identity_criterion)\n",
    "    identity_loss_B, _ = get_identity_loss(real_B, gen_AB, identity_criterion)\n",
    "    identity_loss = identity_loss_A + identity_loss_B\n",
    "    # Cycle-consistency Loss -- get_cycle_consistency_loss(real_X, fake_Y, gen_YX, cycle_criterion)\n",
    "    cycle_consistency_loss_BA, _ = get_cycle_consistency_loss(real_A, fake_B, gen_BA, cycle_criterion)\n",
    "    cycle_consistency_loss_AB, _ = get_cycle_consistency_loss(real_B, fake_A, gen_AB, cycle_criterion)\n",
    "    cycle_consistency_loss = cycle_consistency_loss_BA + cycle_consistency_loss_AB\n",
    "    # Total loss\n",
    "    gen_loss = adversarial_loss + lambda_identity * identity_loss + lambda_cycle * cycle_consistency_loss\n",
    "\n",
    "    return gen_loss, fake_A, fake_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CycleGAN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import color\n",
    "import numpy as np\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "\n",
    "\n",
    "def train(save_model=False):\n",
    "    mean_generator_loss = 0\n",
    "    mean_discriminator_loss = 0\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    cur_step = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Dataloader returns the batches\n",
    "        # for image, _ in tqdm(dataloader):\n",
    "        for real_A, real_B in tqdm(dataloader):\n",
    "\n",
    "            real_A = nn.functional.interpolate(real_A, size=target_shape)\n",
    "            real_B = nn.functional.interpolate(real_B, size=target_shape)\n",
    "            cur_batch_size = len(real_A)\n",
    "            real_A = real_A.to(device)\n",
    "            real_B = real_B.to(device)\n",
    "\n",
    "            ### Update discriminator A ###\n",
    "            disc_A_opt.zero_grad() # Zero out the gradient before backpropagation\n",
    "            with torch.no_grad():\n",
    "                fake_A = gen_BA(real_B)\n",
    "            disc_A_loss = get_disc_loss(real_A, fake_A, disc_A, adv_criterion)\n",
    "            disc_A_loss.backward(retain_graph=True) # Update gradients\n",
    "            disc_A_opt.step() # Update optimizer\n",
    "            \n",
    "            ### Update discriminator B ###\n",
    "            disc_B_opt.zero_grad() # Zero out the gradient before backpropagation\n",
    "            with torch.no_grad():\n",
    "                fake_B = gen_AB(real_A)\n",
    "            disc_B_loss = get_disc_loss(real_B, fake_B, disc_B, adv_criterion)\n",
    "            disc_B_loss.backward(retain_graph=True) # Update gradients\n",
    "            disc_B_opt.step() # Update optimizer\n",
    "\n",
    "            ### Update generator ###\n",
    "            gen_opt.zero_grad()\n",
    "            gen_loss, fake_A, fake_B = get_gen_loss(\n",
    "                real_A, real_B, gen_AB, gen_BA, disc_A, disc_B, adv_criterion, recon_criterion, recon_criterion\n",
    "            )\n",
    "            gen_loss.backward() # Update gradients\n",
    "            gen_opt.step() # Update optimizer\n",
    "\n",
    "            # Keep track of the average discriminator loss\n",
    "            mean_discriminator_loss += disc_A_loss.item() / display_step\n",
    "            # Keep track of the average generator loss\n",
    "            mean_generator_loss += gen_loss.item() / display_step\n",
    "\n",
    "            ### Visualization code ###\n",
    "            if cur_step % display_step == 0:\n",
    "                print(f\"Epoch {epoch}: Step {cur_step}: Generator (U-Net) loss: {mean_generator_loss}, Discriminator loss: {mean_discriminator_loss}\")\n",
    "                show_tensor_images(torch.cat([real_A, real_B]), size=(dim_A, target_shape, target_shape))\n",
    "                show_tensor_images(torch.cat([fake_B, fake_A]), size=(dim_B, target_shape, target_shape))\n",
    "                mean_generator_loss = 0\n",
    "                mean_discriminator_loss = 0\n",
    "            cur_step += 1\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "charchit_irt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
